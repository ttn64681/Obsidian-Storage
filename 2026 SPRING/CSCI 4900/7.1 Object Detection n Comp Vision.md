```table-of-contents
```

## Object Detection
- given an image, an object detection model can identify which of a known set of objects might be present and provide information about their positions w/in the image
- Object detection is different from classification, where we need to classify a single object and determine the location of this object in the image

## Dataset - Object Detection
- in image classification, you have AMNES and CIFAR dataset (relatively easy dateset)

## Dataset - MS CoCo
- more complex dataset
- has captioning (image : description pair)
	- generate a sentence to describe an image
	- similar to Vision Language Model
- has keypoints (similar to skeleton rig)
	- network predicts skeleton keypoints and connects them to draw skeleton

# Terminology in Computer Vision Tasks
- in image classification tasks, we measure using precision/accuracy
- For object detection and segmentation, we measure using **"Mean Average Precision"**

1. Confusion Matrix
2. sad
3. ads
4. Recall

- **mAP:** mean average precision
- **AP:** average precision (for one class)
- **Precision:** the ability of a model to identify **only** the relevant objects
	- out of anything it deemed a positive
- **Recall:** the ability of a model to find all the relevant cases (all ground truth bounding boxes)
	- out of everything
- **Confidence Score:** how conf we are finding an obj in our predicted bounding box
	- we only consider one class at each time when calculating the AP

- TP/TN/FP/FN = True positive, True negative, False pos, False neg

- Precision = TP  /  TP+FP
- Recall = TP /  TP + FN

- precision and recall can contradict each other
- recall is more robust

## Terminology Ex: Ducks
- **when score threshold = 0.5**
- TN = 3
- FP = 1
- FN = 1

- Confidence Score: how conf we are finding an obj in our predicted bounding box
	- we only consider one class at each time when calculating the AP, *so the 'object' here is referring to ducks*

- Precision =?
	- ¾ = .75
- Recall = ?
	- ¾ = .75


- **when score threshold = 0.7**
- TN = 2
- FP = 0
- FN = 2

- Precision =?
	- 2/2 = 1
- Recall = ?
	- 2/4 = .5


## Terminology - IoU (Intersection over Union)
- besides Confident Score threshold, there is also IoU threshold, which can affect the TP and FP.
- for confidence score, the boxes under the threshold are considered not to exist at all. So the confidence score can affect TP, FP, and FN.

- How to indicate or measure whether a prediction is accurate?
	- naive way: overlap the image pixels, but what if the size of image is just different aspect ratio?

- use **IOU: Intersection over Union**
	- B1 n B2 / B1 u B2

- need to set IOU threshold
	- this is also a hyperparameter

- IOU = 0.7
	- TP = 2
	- FP = 1
	- FN = 2

- don't need to remember these, just understand and get a sense why people designed these metrics


### How to calc precision-recall curve
- calc area under curve


- put every predicted bbox into a table, and collect confidence score, TP and FP
- don't need to count FN b/c TP + FN = num ground truth


## **Terminology-NMS (Non-Maximum Suppression)**
- **NMS (Non-Maximum Suppression)**
- we only get the most accurate bounding boxes
	- filter out bad quality bounding boxes

- *Pick B_BOX w/ highest confidence* 
	- *note as B_BOX1*

- *if IoU (B_BOX1, B_BOX2) > threshold*
	- *remove B_BOX2*

-  if bounding boxes overlap with high IoU, probably referring to same object
	- remove lower confidence score of bounding box
- if bounding boxes overlap with low IoU, probably referring to different object
	- NOTE: this is a bottleneck performance to check this:
		- even slower than our first FCL image classification network!

- selecting the bounding box w/ highest confidence
- bounding boxes that are close to selected one are suppressed and merged greedily
- next top-scoring one is selected, and the procedure is repeated until no more bounding boxes remain
- this can be very slow



# First intro of CNN to object detection: R-CNN
- Main features:
	- selective search
	- A natural combination of heuristic region proposal method and ConvNet feature extractor
	- Dataset: Classic object detection extract manual pre-defined features in each region.
		- Classification: ImageNet ILSVC 2012, 1000 classes;
		- Detection: PASCAL VOC 2007, 20 object classes w/ locations, fine-tuning, 1 class background

## R-CNN
- Candidate Region Generation
	- ~2k candidate regions in 1 picture (selective search)
		- selective search is not a Deep Learning method, but a 
		- conventional computer vision method to check color change
		- Proposed regions are cropped and warped to a fixed-size 227x227 image
- Feature Extraction
	- apply AlexNet on each region: 4096 features fc7
- Classification
	- SVM classifier given extracted features
- Bounding Box Regression
	- Use regression to amend the bounding box location

- Problem, we have to go through this 2000 times!

- second warp -> Spp-net & fast R-CNN

## Spatial Pyramid Pooling (SppNet)
- image w/ 224x224, after CONV layers , output: 13\*13\*256
- ...
- ...
- ...

## Fast R-CNN
- go thru nn first
- Contribution:
	- Region-of-Interest (RoI: Given boundaries on an image of an object or on a drawing) from same sampled image share computation
	- Joint Training: Train bbox offset and classifier (using softmax) at same time in one stage using multitask loss

## Faster R-CNN
- Fater R-CNN = fast R-CNN + RPN
- Implement region proposal using CNN on GPU, complete end-to-end model

- **Propose Region-Proposal-Network (RPN)**
	1. A small ConVNet looking at global feature volume in the sliding window
	2. Each sliding window has 9 anchor boxes;
	3. Bounding box regression and box confidence scoring

- if you don't give neural network any hints, makes it very hard to find exact coordinates of 

- *use neural network to find regions of interest*
- *invented anchor boxes*
	- people prefer to use these


# HW + future assignments
- always need to find the bottleneck

# Where are we?
- on PASCAL VOC dataset
- R-CNN 49s/image 66% mAP in ECCV 2014
- Fast R-CNN 3s/image 70% mAP in ICCV 2015
- Faster R-CNN .2s (5fps) image 73.2% mAP in NIPS 2015

- FastRCNN takes like forever for inference (49 seconds i think)
- SPPNet takes 4.3 seconds for inference
- Faster RCNN takes 0.2 seconds for inference
	- videos, games (60Hz - every sec, monitor displays 60 frames, so higher refresh rate means video is smoother)
	- if we draw bounding box on smooth video, then the neural network needs to at least be 60Hz or 120Hz to match refresh rate
	- otherwise, bounding box will have delay on video

- Let's get faster

- important for autonomous driving where it requires real time response to detect bounding boxes quickly

- people invented YOLO

# YOLO (You only look once)
- no longer uses selective search method
- divides image into X x X grid
- difference b/w 2 stage object detection models vs YOLO

- Main feature:
	- Fast, 45fps on NVIDIA Titan X 63.4% mAP (fast YOLO, 9 conv 155 fps, 52.7%)
	- Use complete image as context, rarely mis-predict background as object
	- Good generalization when applied to new domains or unexpected inputs

## YOLO Network Design: A Single Pass Approach

**1. Divide and Conquer (Grid System):**

- The input image is divided into an `S x S` grid (e.g., `7 x 7` in YOLO v1).
- Each grid cell is responsible for detecting objects whose center falls within that cell.

**2. Simultaneous Predictions per Cell:**

- For each grid cell, the network predicts several things directly:
    - **Bounding Boxes:** `B` bounding boxes (e.g., `2` per cell in YOLO v1). Each box predicts:
        - **Coordinates:** `(x, y, w, h)` (center, width, height).
        - **Confidence Score (`C`):** Probability that the box contains an object (`P(Object)`) multiplied by the Intersection Over Union (IoU) between the predicted box and the ground truth box if an object is present. `C = P(Object) * IoU_pred_truth`.
    - **Class Probabilities (`P`):** `C` conditional class probabilities (`P(Class_i | Object)`) for each specific class `i`, _given that an object exists in the cell_.

**3. Output Interpretation:**

- The network's final output for each grid cell is a vector: `(B * 5 + C_classes)` values.
    - For `B` bounding boxes, each box has `5` predictions (`x, y, w, h, Confidence`).
    - Plus `C_classes` class probabilities.
    - Example: `7x7` grid, `2` boxes/cell, `20` classes yields `7 x 7 x (2*5 + 20) = 7 x 7 x 30` output tensor.

**4. Post-processing for Final Detections:**

- **Confidence Thresholding:** Boxes with confidence below a threshold are discarded.
- **Non-Maximum Suppression (NMS):** Eliminates redundant overlapping bounding boxes, keeping only the most confident detection for each object.

### Disadvantages (from YOLO v1):

- **Limited Detections per Grid Cell:** Struggles with multiple small, close-together objects in a single grid cell.
- **Bounding Box Bias:** One of the `B` bounding boxes in a cell can become dominant, limiting the learning opportunity for others.

- 2 bounding boxes assigned to each grid, but only 1 is taken
- one bounding box may be marked, but not contribute to loss
- can cause bias:
	- one bias gets more powerful, since it always performs better than red one
	- so green one will always be trained while red will not be trained or have chance to learn


# YOLO Training
- YOLO is trained end-to-end using a multi-part loss function. The total loss combines several terms:
- Loss function
- Line 1 and 2: coordinate prediction
- Line 3: confidence prediction in bbox containing an object
- Line 4: confidence prediction in bbox not containing an object
- Line 5: class prediction
- lambda is balancing factors:
	- If an image has many bbox w/o object, confidences are 0 for them which makes network not stable or diverge. lambda = 0.5

- small bbox offset is more significant than big ones

- YOLO v1 can only detect 2 things w/in a single grid, since only 2 bounding boxes

# YOLO Inference & Limitation

# SSD: Single Shot MultiBox Detector

SSD combines the speed of single-shot detectors (like YOLO) with the accuracy benefits of anchor boxes (similar to Faster R-CNN's RPN) and, crucially, **multi-scale feature maps** to detect objects of varying sizes effectively.

- **"Combining these 2"**: This likely refers to combining the "single-shot" approach (meaning it does detection in one pass, like YOLO, without separate region proposal steps) with the use of "anchor boxes" (predefined boxes of various aspect ratios, similar to Faster R-CNN's approach, which help the network predict more accurate bounding boxes).
    
- **"It can capture any size"**: This is achieved through SSD's use of **multi-scale feature maps**.
    
    - Unlike YOLO v1, which made predictions only from the final feature map, SSD makes predictions from _several_ feature maps at different layers of its network.
    - **Base Network (e.g., VGG-16)**: SSD typically starts with a pre-trained image classification network like VGG-16. It uses the initial layers (e.g., up to `conv5_3` in VGG-16) as its base for extracting rich semantic features.
    - **"Extra features" (Additional Layers)**: SSD then adds several custom convolutional layers on top of the base network. These additional layers progressively decrease in spatial resolution but increase in semantic depth.
    - **Detection at Multiple Scales**:
        - The earlier, high-resolution feature maps (like `conv4_3` or `conv5_3` from VGG-16) are good for detecting **small objects** because they retain fine-grained spatial details.
        - The later, low-resolution feature maps (from the added extra layers) are good for detecting **large objects** because they capture broader contextual information.
    - For each of these multi-scale feature maps, SSD predicts a set of default boxes (anchors) and then refines their coordinates and predicts class probabilities.

By making predictions at different resolutions throughout the network, SSD ensures that objects of a wide range of sizes can be accurately detected.

# Test Notes:
- both conf score and IoU affect precision and recall
	- how to calc precision and recall
		- AP = area under precision-recall curve
			- this is just for 1 class
				- every class has its own average precision
		- mean average precision is over all class data sets
	- AP equals to the area under recall curve



