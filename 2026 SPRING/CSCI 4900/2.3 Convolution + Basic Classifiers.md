```table-of-contents
```
# Previously...
- Supervised vs Unsupervised Learning
- 3 datasets:
	- training
	- validation
	- testing
- Feed-forward Neural Networks:
	- no loop/cycles
		- unidirectional
	- if you have connection from input unit to output unit directly,
		- still a feed-forward network
	- Ex:
		- Transformers
- Fully-Connected Layer (aka Dense Layer)
	- in pytorch, its called the "Linear Layer"

# Convolutional Neural Network (CNN)
## What is Convolution
- a mathematical op that slides a small filter over data to extract important local patterns, such as edges or textures in an image
	- extract patterns, edges, textures...
	- creates a feature map
	- similar to image processing

- Convolutional Neural Network (CNN, or ConvNet) is a class of deep, feed-forward artificial neural networks that has successfully been applied to analyzing visual imagery
- LeNet - the first CNN
	- Black and White Image 
## LeNet Process
- Convolutions extract features and creates Feature Maps
- Subsampling layer - shrink size of feature Map
	- og 28x28
	- shrink by 2 -> 14x14
- Another convolution layer, keeps extracting features and making features maps
- Another subsampling layer -> 5x5
- Fully connected layer -> Output layer
- How to fit image to full connected layer?

Essentially:
- flatten image, make it long vector, and fit it into fully connected layer:
	1. after going thru convolution layer, intermediary result is the feature map
	2. 6 feature maps
	3. shrink size of feature map in half
	4. repeat

- from low-level features to high-level features

- if 120 input neurons and 85  output neurons, **how many weights in fully connected layer?**
	- Weights: 120 x 85
	- This is the size of fully connected layer
	- generally use fully connected layer at end of a convolution, for classification

- guess the accuracy of classification of this: 98-99% 

- design decision: how many convolution layers and fcl's?

- 10 classes, 10 output digits, so 10 neurons to represent 10 classes

## VGG16 CNN Model

## AlexNet
- important milestone neural network
- ImageNet: 2012 challenge/competition, compare accuracy
- AlexNet is winner

## Another way to show the Model
- Show features maps vs Show layer names
	- Show layer names is more common

## Computer Vision Milestones:
- Year 2010: NEC-UIUC
- Year 2012: SuperVision
- Year 2014: GoogLeNet, VGG
- Year 2015: MSRA

# Recurrent Neural Networks (RNN)
- different from feed-forward neural networks
- There are directed cycles in their connection graph
	- **can actually loop-back to previous layers**
	- *can go back to where you started by following arrows*
- hard to train these networks
	- They can have complicated dynamics

- Recurrent neural networks are a very natural way to model sequential data:
	- e.g., text, speech, time series
- Traditional feed forward networks process each input independently
- RNN's have a "memory" that allows them to consider previous inputs in a sequence
	- They are equivalent to very deep nets with one hidden layer per time slice

## Some Important People in DNN
- Geoff Hinton - Google
	- backpropagation, boltzmann machines
- Yann Lecun was part of Meta - Facebook
	- convolution
- Yoshua Bengio - Godfather of AI and Deep Learning
	- first whose works we're cited by people over 1mil time
	- stacked auto-encoders
- Andrew Ng - DeepLearning.AI
	- GPU utilization
- Alex Krizhevsky - Google
- Fei-Fei Li
	- Establishing ImageNet
	- data is everything for DeepLearning!!!!
- Ian goodfellow - Inventory of GAN (Generative Adversarial Neural Network)
- - Sam Altman - CEO of OpenAI
	- just a businessman... lmao
- Jensen Huan - Founder & CEO of NVIDIA


- XBox - had higher computation power in the past due to GPU


# Notes

# Vocab
