```table-of-contents
```
# Course Material: Midterm & Final Exam Topics

## Key Concepts to Master

*   **Network Architectures**:
    *   Recognize and differentiate between LeNet, VGGNet, ResNet, and DenseNet structures.
    *   Understand the defining characteristics and architectural innovations of each.
*   **Evaluation Metrics**:
    *   Understand the "Top 5 Error" metric and its implications.
*   **Batch Normalization**:
    *   Explain its role in stabilizing and accelerating Deep Neural Network (DNN) training.
    *   Comprehend the mechanisms that mitigate unstable gradients.

# Batch Normalization (BN) Explained

## Core Purpose

Batch Normalization improves training by **stabilizing and accelerating** deep neural networks. It combats **Internal Covariate Shift**.

## Internal Covariate Shift

*   **Definition**: The change in the distribution of a layer's inputs during training.
*   **Cause**: As network parameters are updated via backpropagation, the output distribution of preceding layers changes, altering the input distribution for subsequent layers.
*   **Impact**: Slows down training and necessitates careful hyperparameter tuning (learning rates, initialization).

## How Batch Normalization Works

BN applies normalization, scaling, and shifting to the inputs of a layer.

1.  **Normalization Step**:
    *   Ensures that the inputs to a layer have a mean of 0 and a variance of 1 for each mini-batch.
    *   **Formula**: $\hat{x}^{(k)} = \frac{x^{(k)} - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}$
        *   $x^{(k)}$: Input to the layer.
        *   $\mu_B$: Mean of the current mini-batch.
        *   $\sigma_B^2$: Variance of the current mini-batch.
        *   $\epsilon$: Small constant for numerical stability.
    *   **Mini-batch Statistics**: $\mu_B$ and $\sigma_B^2$ are calculated per mini-batch during training.
    *   **Running Statistics**: `running_mean` and `running_variance` are maintained as exponential moving averages of batch statistics. These are *not* trainable.

2.  **Scale and Shift Step**:
    *   The normalized output ($\hat{x}^{(k)}$) is scaled by a learnable parameter $\gamma$ (gamma) and shifted by a learnable parameter $\beta$ (beta).
    *   **Formula**: $y^{(k)} = \gamma \hat{x}^{(k)} + \beta$
    *   **Learnable Parameters**: $\gamma$ and $\beta$ are learned via backpropagation. They allow the network to adjust the normalized values, potentially restoring the original representation if beneficial.

## Usage During Training vs. Inference

*   **Training Mode (`model.train()`)**:
    *   Uses mini-batch statistics ($\mu_B, \sigma_B^2$) for normalization.
    *   Updates `running_mean` and `running_variance`.
    *   Allows $\gamma$ and $\beta$ to be learned.
*   **Inference Mode (`model.eval()`)**:
    *   Uses fixed `running_mean` and `running_variance` for normalization (ensures deterministic output).
    *   Does *not* update running statistics.
    *   Applies the learned $\gamma$ and $\beta$.

## Key Benefits of Batch Normalization

*   **Faster Convergence**: Enables higher learning rates.
*   **Improved Gradient Flow**: Mitigates vanishing and exploding gradients.
*   **Regularization**: Provides a mild regularization effect.
*   **Initialization Robustness**: Makes the network less sensitive to initial weight values.

## Understanding Gradients (Vanishing & Exploding)

*   **Gradients**: Indicate how changes in weights affect the network's error. Essential for learning via backpropagation.
*   **Vanishing Gradients**: Gradients become infinitesimally small during backpropagation through deep networks. Early layers learn extremely slowly or not at all. Caused by repeated multiplication of small numbers (e.g., derivatives of saturating activation functions like sigmoid).
*   **Exploding Gradients**: Gradients become excessively large, leading to unstable, erratic weight updates and training divergence. Caused by repeated multiplication of large numbers.
*   **BN's Role**: By normalizing layer inputs, BN helps keep activations and their gradients within a more stable range, preventing them from becoming too small or too large.

# Deep Dive into CNN Architectures

## Foundational Concepts

*   **Activations**:
    *   **Definition**: The output values of neurons after an activation function is applied. They represent the computed features or information passed to the next layer.
    *   **Activation Functions (e.g., ReLU, Sigmoid)**: Introduce non-linearity, enabling networks to learn complex patterns.
*   **Convolutional Layers**: Apply learnable filters to detect spatial hierarchies of features (edges, textures, etc.).
*   **Pooling Layers**: Reduce the spatial dimensions (width, height) of feature maps, decreasing computation and parameters while retaining key information.
*   **Fully Connected (FC) Layers**: Neurons connect to all neurons in the previous layer. Typically used at the end for classification.
*   **Global Average Pooling (GAP)**: A technique to replace FC layers. It averages each feature map to a single value, creating a fixed-size output (1x1xC). Benefits: dramatically reduces parameters, combats overfitting, and allows variable input sizes.

## Architectures: Recognition and Characteristics

### 1. LeNet-5

*   **Structure**: Early, relatively shallow architecture.
*   **Pattern**: Sequential application of Convolution -> Pooling -> Convolution -> Pooling -> FC -> FC. Often uses Average Pooling.
*   **Use Case**: Primarily digit recognition.

### 2. VGGNet

*   **Structure**: Deep network characterized by **uniformity** and **depth**.
*   **Pattern**: Uses stacks of small (3x3) convolutional layers, followed by a Max Pooling layer. The number of filters typically doubles after each pooling step.
*   **Recognition**: Look for repeating blocks of 2-4 Conv layers followed by a Max Pooling layer.

### 3. ResNet (Residual Network)

*   **Structure**: Introduces **residual blocks** with **skip connections**.
*   **Key Innovation**: Skip connections allow the input of a block to be added directly to its output. This facilitates training of very deep networks by easing gradient flow.
*   **Recognition**: Identify connections that bypass layers and merge back via addition.

### 4. DenseNet (Densely Connected Network)

*   **Structure**: Each layer is **densely connected** to all preceding layers within a "dense block."
*   **Key Innovation**: Feature maps from all previous layers are concatenated and fed into the current layer. Promotes feature reuse and reduces parameters.
*   **Recognition**: Look for concatenation operations where outputs from multiple distinct prior layers are combined.

## Evaluation Metrics

*   **Top-5 Error**: The percentage of test cases where the correct label is *not* among the model's top 5 highest-confidence predictions.
*   **Top-1 Accuracy**: The percentage of test cases where the model's single most confident prediction is correct.

# Hardware Considerations

*   **Computation Cost (FLOPs)**: Convolutional layers often dominate FLOPs due to the sliding window operations, even with fewer parameters than FC layers.
*   **Memory Usage**: Deep networks with many filters require substantial memory for storing activations and weights.

# Practical Implementation Notes

## Mode Switching (`.train()` vs. `.eval()`)

*   **`model.train()`**: Activates **training mode**.
    *   **Dropout**: Randomly zeroes activations (regularization).
    *   **Batch Normalization**: Uses mini-batch statistics; updates running statistics.
*   **`model.eval()`**: Activates **evaluation mode**.
    *   **Dropout**: Disabled; all activations are used.
    *   **Batch Normalization**: Uses fixed running statistics; does not update them.

## Layers with Mode-Dependent Behavior

*   **Dropout Layer**:
    *   **Training**: Randomly zeros out activations to prevent co-adaptation.
    *   **Inference**: All activations are kept (effectively scaled by the dropout rate).
*   **Batch Normalization Layer**:
    *   **Training**: Normalizes using current batch statistics; updates running statistics.
    *   **Inference**: Normalizes using stored running statistics.
