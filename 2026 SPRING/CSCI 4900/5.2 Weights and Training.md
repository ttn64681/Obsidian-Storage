```table-of-contents
```
# Weight Intialization
- Weights canNOT be initialized to 0:
- Why?
	- *there still will be discrepancy in output layer due to ground-truth layers*
	- intermediary layer nodes will all = 0 (since multiply by 0)

- partial derivative / gradient of weight is = input
- partial derivative / gradient of local input is

- your model will not move anymore
- gradient will be 0

## Large Random Numbers
- init weight using large random nums
- almost all neurons saturated at either -1 or +1
- gradient will STILL be 0, even with Sigmoid activation function
- Why?
	- look at f(x)(1 - f(x))
		- result will still be 0


- either information become very small or very large, with no distribution (intermediary result)
- or there will be no information passed to network
### Xavier Initialization
- More fan in, the smaller weight should be
	- So that the activation won't saturate (become -1 or +1)
- **`W = np.random.randn(fanin, fanout) / np.sqrt(fan_in)`**

- information will neither be very small nor very huge
- if huge: gradient exploding problem
- if small: gradient vanishing problem

- we want to **keep variance between the layers the same**
- if variance is huge, curve will be very wide
- if variance is small, curve will be very narrow

- this variance is related to number of layers and weights
- that's why, when initializing weight magnitude, need to consider the weights of each layer

- **Key takeaway:**
	- **Distribution of initial weights should relate to the number of input/weights**

- Initial Standard Initialization, more layers would cause more activation saturation (gradient vanishing)
	- could only support up to 13
- With Xavier Initialization, we can add more layers
	- this is good for Sigmoid and TanH
	- for ReLU, it's capped at negative part
		- this will change the variance of the ___
		- **`W = np.random.randn(fanin, fanout) / np.sqrt(fan_in / 2)

## Kaiming Initialization
- Xavier Initialization
	- W = np.random.randn(fanin, fanout)/np.sqrt(fan_in)
	- W= np.random.randn(fanin, fanout)/np.sqrt(fan_in/2)

	- W= np.random.randn(fanin, fanout)/np.sqrt(2/fan_in)
		- this is most common weight initialization method nowadays


# Naive Weight Update
- Stochastic gradient descent
- **`W += -learning_rate * dW`**
- Magnitude of W depends on learning rate and gradient, and perpendicular to error surface
	- When error surface is elongated, **it zig-zag its way to bottom**
	- Not efficient
	- Should not change direction drastically
	- this makes training slower

## AdaGrad Update
- cache += dW^2
- W += l-learning_rate\*dW / (np.sqrt(cache) + 1e-7)
- Previous approach uses same learning rate (step size) for all weights
	- Large learning rate for weight that has steep gradient may cause over correct
	- Small learning rate for weight that has gentle gradient converges too slow
		- *+1e-7 is just to prevent denominator from being 0*

- this doesn't seem to make sense?
- but empirically it works

## RMSProp Update
- instead of keep adding gradient squared to cache..... ?
- instead of making cache larger and denominator smaller, it has an exponential moving average (EMA) of the gradient

- cache is an exponential moving average (EMA) of the gradient
	- cache(t) = (1 - decay_rate) [dW(t-1) +
	decay_rate * dW (t-2) + d..... ]

- each individual weight has its own momentum and cache
- Adam causes 4-6x more memory consumption compared to weights
	- so if W 1GB, the optimizer will take 4-6GB
- This is why training more mem intensive
- not only store intermediary results and gradients, but also due to optimizer
- DURING INFERENCE, this doesn't matter, just discard these values

## Adam Update
1. Momentum
2. RMSProp (Adagrad)
3. Adam

- memory consumption
- each of them has same shape as weight of matrix

# Learning Rate
![[Drawing 2026-02-12 16.50.13.excalidraw]]
- learning rate decays over time
	- step decay: decay learning rate by half every few epochs
	- Exponential decay
		- alpha = alpha_0 e^-kt
	- 1/t decay
		- alpha = alpha_0 / (1 + kt)

- you can also measure it by loss, checking discrepancy between epochs, and if its smaller than certain threshold, you decay the learning rate




## Hyperparameter Optimization
- include network architecture
- learning rate, decay rate, update type
- regularization

- for new people, just search on github if people have empirical values
- in research, researchers are required to provide their hyperparameter setting
	- what are the weights
	- how many epochs

# Dropout technique
- Good way to mitigate overfitting problem
- In intermediary results (not on the weights), for each neuron, you have 50 percent probability to omit a hidden unit
- each unit has independent dropout probability
- randomly select what we will discard

- so we randomly sample from 2^H diff architectures
	- All architectures share weights
	- Every model is very strongly regularized

- Each model is trained using different training samples

## An intuitive way to think Drop Out
![[Drawing 2026-02-12 17.01.36.excalidraw]]


## To Implement Dropout
```
H1 = np.max (0, np.dot(W1, X) + b1)
U1 = np.random.rand(*H1.shape)<p # <- * unpacks tuple into positional args
H1 *= U1 #drop
H2 = np.maximum (0, np.dot(W2, H1) + b2)
U2 = np.random.rand(*H2.shape)<p
H2 *= U2 #drop
```

- **Model Pruning:** is removing the redundant weights
	- not all features are important
- Drop out is different:

- **Dropout happens during inference,**
- **Pruning happens during weights/training**

## Early Stop
- **Early stop can be used to avoid overfitting**
- Overfitting happens when accuracy of the training set is going up while the accuracy of the validating set is going down
	- A good time to terminate the training
# Notes
- the learning rate, decay, and batch size are called Hyperparameters
- the model weights are called model parameters

- **Dropout happens during inference,**
- **Pruning happens during weights/training


# Vocab:
- **Model Pruning:** is removing the redundant weights
	- not all features are important
	- different from dropout