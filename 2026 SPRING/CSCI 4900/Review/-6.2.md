```table-of-contents
```

### Phase 1: The Intuition and Biological Beginnings 

Deep Learning didn't just appear out of nowhere; it's an attempt to emulate the biological neural systems of the human brain. The foundational idea comes from the **Hubel & Wiesel experiment** on the visual cortex, which showed that neurons process visual information hierarchicallyâ€”first detecting simple edges (low-level), then shapes (mid-level), and finally complex objects (high-level).

To emulate this, we created the **Artificial Neuron**. A single neuron takes inputs ($x$), multiplies them by weights ($w$), adds a bias ($b$), and passes the result through an **Activation Function** to introduce non-linearity.

- **Without activation functions, a neural network, no matter how deep, is just a giant linear regression model.**
    

**Key Activation Functions you need to know:**

- **Sigmoid**: $y = \frac{1}{1 + e^{-x}}$. Squashes values between 0 and 1. (Derivative: $f(x)(1 - f(x))$).
    
- **Tanh**: Similar to sigmoid but zero-centered (-1 to 1)..
    
- **ReLU (Rectified Linear Unit)**: $max(0, x)$. Solves vanishing gradients for positive values and is computationally cheap.
    

---

### Phase 2: Classifiers, Loss, and How We Learn (Tie-in to HW 1) 

In your first homework, you were tasked with building a neural network classifier using the **CIFAR-10 dataset** (images of size $32\times32\times3$) using only Fully Connected (Dense) layers.


To make a model learn, we need three steps:

1. **Forward Propagation**: The model makes a guess. For classification, we use the **Softmax** function at the output layer to turn raw model scores into probabilities that sum to 1.
    
    - _Formula_: $P(Y=k|X=x) = \frac{e^{s_k}}{\sum_j e^{s_j}}$.
        
2. **Loss Function**: We measure how wrong the guess was. For Softmax, we almost always use **Cross-Entropy Loss (Negative Log-Likelihood)**. _(Note: As your HW1 states, PyTorch combines Softmax and Cross-Entropy into one function!)_.
    
    - _Formula_: $L_i = -log(\frac{e^{s_{y_i}}}{\sum_j e^{s_j}})$.
        
3. **Backward Propagation & Gradient Descent**: We use the **Chain Rule** of calculus to figure out how much each weight contributed to the error, and then update the weights by stepping in the opposite direction of the gradient.
    
    +1
    
    - _Formula_: $w_{new} = w_{old} - (learning\_rate \times gradient)$.
        

**Optimizers:** Standard Stochastic Gradient Descent (SGD) can get stuck in local minima. To fix this, we use advanced optimizers like **Momentum** (which builds up speed in consistent directions) and **Adam** (which adapts the learning rate for each individual weight based on past gradients). HW1 specifically suggests using Adam!


---

### Phase 3: The "Black Magic" of Deep Networks 

As networks get deeper, they become incredibly hard to train. The gradients can either shrink to zero (**Vanishing Gradients**) or blow up to infinity (**Exploding Gradients**). Here is what you need to know for your tests to solve these issues:

#### 1. Weight Initialization

- **Never initialize to zero!** All neurons will learn the exact same features because their gradients will be identical.
    
- **Xavier Initialization**: Best for Tanh/Sigmoid. It keeps the variance of activations consistent across layers. $W \sim U(-\frac{\sqrt{6}}{\sqrt{n_{in}+n_{out}}}, +\frac{\sqrt{6}}{\sqrt{n_{in}+n_{out}}})$.
    
- **Kaiming (He) Initialization**: Best for ReLU. It accounts for the fact that ReLU zeroes out half the data. $W \sim N(0, \frac{2}{n_{in}})$.

#### 2. Regularization (Fighting Overfitting)

When your training accuracy is high but validation accuracy is low, you are overfitting.

- **L1/L2 Regularization**: Adds a penalty to the loss function for having large weights.
    
- **Dropout**: Randomly turns off a percentage of neurons during _training_ (e.g., $p=0.5$). This forces the network not to rely on any single feature. During _inference/testing_, you turn Dropout off.

#### 3. Batch Normalization (BN)

This is a massive test topic. BN standardizes the inputs to a layer across a mini-batch to have a mean of 0 and variance of 1, combating **Internal Covariate Shift**.

- _Test specific_: During training, BN uses the _current batch's_ mean and variance. During inference, it uses a fixed _exponential moving average_ calculated during training.
    
- **BN Folding**: For speed during inference, the math of the BN layer can actually be "folded" or absorbed directly into the weights of the preceding Convolutional layer.
    

---

### Phase 4: Convolutional Neural Networks (CNNs) 

In HW1, you couldn't use Conv layers, meaning you had to flatten the $32\times32\times3$ image into a massive 3072-dimension vector. If you add deep Fully Connected layers, the number of parameters explodes (Millions of weights!).

**Convolutions** solve this by using small, sliding filters (kernels) that share weights across the whole image.

#### The "Must-Know" CNN Formulas for Tests:

Given an Input Size ($W_1$), Filter Size ($F$), Stride ($S$), Padding ($P$), and Number of Filters ($K$):

1. **Output Size Dimension**:
    
    $W_2 = \frac{W_1 - F + 2P}{S} + 1$.
    
2. **Number of Parameters**:
    
    $(F \times F \times Depth_{in} + 1_{bias}) \times K_{filters}$.
    
3. **Computation Cost (MACs - Multiply-Accumulate)**:
    
    $F \times F \times Depth_{in} \times K_{filters} \times W_{out} \times H_{out}$.
    

#### CNN Architecture Evolution:

- **LeNet (1998)**: The pioneer. Simple Convs and Average Pooling.
    
- **AlexNet (2012)**: Proved deep learning works. Used ReLU, Dropout, and heavy Data Augmentation.
    
- **VGGNet (2014)**: Proved "deeper is better." Used exclusively small $3\times3$ filters to build very deep networks.
    
- **GoogLeNet/Inception**: Used $1\times1$ convolutions to dramatically reduce dimensionality and parameter counts before doing expensive $3\times3$ or $5\times5$ convolutions.
    
- **ResNet**: Solved the vanishing gradient problem in ultra-deep networks (152+ layers) by using **Skip Connections (Residuals)**. Instead of learning a direct mapping $H(x)$, it learns the residual $F(x) + x$. This gives gradients a "shortcut" to flow backwards perfectly.
    
- **MobileNet**: Optimized for phones. Replaces standard expensive convolutions with **Depthwise Separable Convolutions** (a spatial convolution per channel, followed by a $1\times1$ pointwise convolution), drastically reducing MACs.
    

### Summary for your Homework and Exams

If you understand the math of _why_ we moved from flattening images (HW1) to sliding filters (CNNs), how we stop those networks from dying (Initialization, Batch Norm, Skip connections), and how to calculate the sizes of those tensors as they pass through layers, you will easily ace this class.

Let me know if you want to run through a practice calculation for the Output Size/Parameters formula, as those are guaranteed to show up on your test!