```table-of-contents
```

# Neural Network Training: Forward, Backward, and Gradient Descent

- **Forward Propagation** (making a prediction), 
- calculating the **Loss** (how wrong the prediction was), 
- **Backward Propagation** (calculating how to adjust the network's parameters), and 
- **Gradient Descent** (updating the parameters).

## Forward Propagation (The Prediction/Inference Step)
-   **Purpose**: To take an input and pass it through the network's layers to produce an output (a prediction). This is also known as **Inference**.
-   **Process**:
    -   Start at the input layer.
    -   Each neuron in a layer takes weighted inputs from the previous layer, sums them, adds a bias, and then applies an **activation function**.
    -   This process is repeated layer by layer until an output is generated at the final layer.

## What is Inference?
- Inference (or prediction) is the process of using a _trained_ neural network model to make predictions on _new, unseen data_. This only involves the **Forward Propagation** step. The model takes an input, processes it through its layers, and produces an output.

## Loss Function
-   **Purpose**: To quantify how "wrong" the network's prediction is compared to the actual target value.
-   **Example**: For regression, a common loss function is Mean Squared Error (MSE):
    -   `L = (a - t)^2` (where `a` is the network's output/activation and `t` is the target).

## Backward Propagation (Calculating Gradients)
-   **Purpose**: To calculate the **gradient** (rate of change) of the loss function with respect to each weight and bias in the network. This tells us how much each parameter contributes to the error and in what direction it should be adjusted.
-   **Process**: This uses the **chain rule** of calculus, working backward from the output layer to the input layer.
    -   **Start at the Loss**: Calculate `∂L/∂a` (gradient of loss with respect to the output activation).
    -   **Propagate Gradients through Activation Functions**: For each layer, calculate `∂L/∂z = ∂L/∂a * ∂a/∂z`. (`z` is the weighted sum before activation, `a` is the activation output).
    -   **Calculate Gradients of Weights and Biases**:
        -   The local gradient of any weight `w_ij` (connecting input `x_i` to neuron `j`) is generally `∂L/∂w_ij = ∂L/∂z_j * x_i`.
        -   The local gradient of a bias `b_j` is `∂L/∂b_j = ∂L/∂z_j * 1`.

### Vectorization
-   Neural networks don't calculate every individual weight's gradient separately; they compute gradients for entire layers (or mini-batches) using **matrix/vector operations**. This makes computation much more efficient.
    -   Example: `∂Y = [∂L/∂y1, ..., ∂L/∂y_D]` represents the vector of gradients for an output vector Y.

## Gradient Descent (Updating Parameters)
-   **Purpose**: To adjust the network's weights and biases to minimize the loss function.
-   **Update Rule**: For each weight `w_i` (and similarly for biases):
    -   `w_i = w_i - step_size * ∂L/∂w_i`
-   **Intuition**:
    -   We subtract `step_size * ∂L/∂w_i` because we want to move in the **opposite direction** of the gradient (which points towards increasing loss).
    -   If `∂L/∂w_i` is *positive*, increasing `w_i` increases `L`. We want to decrease `L`, so we subtract, making `w_i` smaller.
    -   If `∂L/∂w_i` is *negative*, increasing `w_i` decreases `L`. We want to decrease `L`, so we subtract a negative, which effectively adds to `w_i`, making `w_i` larger.
    -   `step_size` (aka *learning rate*) controls how large of a step we take in the direction of steepest descent.

## Activation Functions and Their Derivatives
Activation functions introduce non-linearity into the network, allowing it to learn complex patterns. Their derivatives are crucial for backward propagation.

### ReLU (Rectified Linear Unit)
-   **Function**: `f(x) = max(x, 0)`
-   **Derivative**:
    ```
    ∂f(x)/∂x = { 0   if x < 0
               { 1   if x > 0
               { None if x = 0 (conventionally assigned 0 or 1 during training)
    ```
-   **Note**: During training, the derivative at `x=0` is typically assigned 0 or 1.
-   **Issue**: Discards negative information entirely, which can sometimes be important.

### Leaky ReLU
-   **Function**: `f(x) = max(0.01x, x)` (or `ax` for `x<0`, where `a` is a small positive constant like 0.01)
-   **Derivative**:
    ```
    ∂f(x)/∂x = { 0.01 if x < 0
               { 1    if x > 0
               { None if x = 0 (conventionally assigned 0.01 or 1)
    ```
-   **Benefit**: Addresses the "dying ReLU" problem by allowing a small gradient for negative inputs, preserving some negative information.

### Sigmoid
-   **Function**: `f(x) = 1 / (1 + e^-x)`
-   **Derivative**: `∂f(x)/∂x = f(x) * (1 - f(x))`
-   **Note**: Squashes values between 0 and 1, often used in output layers for binary classification. Can suffer from vanishing gradients.

### Tanh (Hyperbolic Tangent)
-   **Function**: `f(x) = (e^x - e^-x) / (e^x + e^-x)`
-   **Derivative**: `∂f(x)/∂x = 1 - f(x)^2`
-   **Note**: Squashes values between -1 and 1, zero-centered. Also prone to vanishing gradients.

### Softmax
-   **Function**: Converts a vector of arbitrary real values into a probability distribution. Often used in the output layer for multi-class classification.
-   **Derivative**: Computationally complex to derive for individual outputs but is usually calculated as part of a combined derivative with the cross-entropy loss function, resulting in a simple expression.

# Computation Costs
-   **Backward Propagation** generally costs about **2 times** the computation of **Forward Propagation**
-   Therefore, the total computation cost of **training** a neural network (Forward + Backward Propagation) is approximately **3 times** the cost of **inference** (only Forward Propagation).

Ex: 3x5 x 5x1
- **what is cost for vector multiplication?**
	- how many **multiplications** do we have?
		- output is 3rows x 1col
		- 5 multiplications for a single row
		- there are 3 rows, so
		- *= 15 Multiplications*
	- how many **additions** do you have?
		- 4 additions per row
		- there are 3 rows, so
		- *= 12 Accumulations*
- 5 x 3 + 4 x 3 = 15 + 12 = **27 FLOPs**
- So what is formula for cost?
	- N x M  x T + (M-1) x N x T = Total FLOPs
		- N - rows first matrix
		- M - cols first matrix
		- T - cols second matrix

- This is the computation cost for neural network inference (single matrix multiplication)

# Memory Consumption

-   **During Inference (Forward Propagation only)**:
    -   Peak memory consumption is **lower** because you can often **discard the intermediate outputs** of each layer once the next layer's input has been computed. Only the final output and current layer's inputs/weights need to be held.

-   **During Training (Forward + Backward Propagation)**:
    -   This is different. During the initial **Forward Propagation** pass:
        -   You **cannot discard intermediary results** (activations, pre-activation values `z`) for each layer.
        -   **Backward Propagation needs these stored values** to correctly compute gradients using the chain rule.

-   **The Trade-off**:
    -   Storing these intermediate values consumes significant memory, especially for deep networks or large batches.
    -   To **save memory** (e.g., on memory-deficient devices like mobile phones or embedded systems with limited RAM and no HDD), a common **trick** is to **recompute** these intermediate values during the backward pass rather than storing them during the forward pass.
    -   This **sacrifices computation time for memory efficiency**. Recomputing means doing extra work, but it avoids the need to store large amounts of data.


# The Error (Loss) Surface

- The **loss surface** is a high-dimensional function that maps the network's parameters (weights and biases) to a single value: the loss.
- **Goal of Training**: To find the set of parameters that minimizes this loss function, effectively finding the "lowest point" on the loss surface.
- **Characteristics**:
    - **Complexity**: The surface can be highly complex, with many **local minima** (points that are lower than their immediate surroundings but not the absolute lowest) and **saddle points** (points that are minima in some directions and maxima in others).
    - **Shape**: The shape of the loss surface is influenced by the network architecture, the data, and the loss function.

# Gradient Descent and Batch Learning
- <span style="background:#fff88f">The ideal learning does steepest descent on error surface</span>
## Batch Gradient Descent:
- update model after computing the gradient w/ respect to **entire** training set
	- so process is: ... calc avg loss to calc gradient, and update model once
	- in other words, we calc weights once when we calc all training data
## Stochastic Gradient Descent:
- update model after computing the gradient w/ respect to a single training ex
	- **Stochastic** means **randomly sampled training data**

- These are NOT ideal... why?
	- consider all data together and update model once
	- different data have different features that can cancel each other out
	- ex: one dog may have short tail, other may have long tail
		- so not a perfect case
	- **Additionally, updates can be very noisy, leading to oscillations around the minimum, and it can converge slower than mini-batch for larger datasets.**

## Mini-Batch Gradient Descent:
- update model after computing the gradient w/ respect to a **subset (small batch)** of the training set
	- Calc avg gradient of a set of training samples
	- More perpendicular to the contour lines
	- Typical mini-batch size is 32/64/128 (CIFAR-10), 512/1024/2048 (ImageNet)

### Trade-offs with Batch Size:
- **Increased Batch Size:**
	- **Pros:** More stable gradient estimates, potentially faster convergence per epoch, can leverage hardware parallelism better.
	- **Cons:** Higher memory consumption (requires storing more activations and gradients), may converge to sharper minima (which can generalize worse), slower progress per update step.
- **Decreased Batch Size:**
	- **Pros:** Lower memory consumption, can escape local minima more easily due to noisy updates, faster progress per update step.
	- **Cons:** Noisier gradient estimates, can oscillate more, may require more updates (iterations) to converge.

- **Memory Consumption Example:**
	- More data = more mem consumption
		- more weights means more activations
		- say activation is 32 times
			- this is done 32x at every activation function
		- say image is 128, all of the values has to be stored at each of the specific intermediary ... (?)
		- these are the model parameters, not related to how much image
		- each weight has its own gradient
		- each image has its loss
		- based on loss we calc gradient of each image
		- then accumulate gradients together to get final Loss along Batch Dimension
		- so too large batch size, your gpu memory may not be able to accomodate such huge size
		- during hw, change batch size (NVIDE-smi) to see how it effects memory consumption

- Results of different types of training methods:
	- Mini-Batch is actually fastest in training under data-set

# Training Mechanics
## Epochs
- " 1 training epoch"
	- training over dataset once
![[Drawing 2026-02-03 17.37.42.excalidraw]]
## Iterations/Steps
- " finished 1 training batch (1 Iteration/Step) "
	- *how many steps to finish 1 epoch?*
	- given Batch Size = 100
	- given epoch = 10,000
	- **Answer: 100**

- this is why training is so MEMORY-INTENSIVE

- these days we can deploy smaller versions of LLM on phones and laptops,
- BUT there's no way to TRAIN the LLM on your phone/laptop/network

# Visualization of Loss Surface
- We prefer a surface with less **local minima** and **saddle points**, leading to a smoother descent towards the global minimum.
- **Surfaces with skip connections are generally preferred over surfaces without skip connections (which tend to have more local minima).**
    - **Skip connections allow gradients to flow more directly through the network, helping to avoid vanishing gradients and navigate the loss landscape more smoothly.**
- 14-layer plain
	- VGG uses this
- 34-layer resnet
	- ResNet
		- developed shortcut to jump and have smoother Loss Surface
## Loss Surface Characteristics (Revisited)
- **Depth vs. Width**: Deeper networks can sometimes have more complex loss surfaces than wider ones.
- **Parameterization**: Different parameterizations of the same function can lead to vastly different loss surfaces.
- **Generalization**: The characteristics of the loss surface (e.g., the prevalence of wide, flat minima) are believed to correlate with the generalization ability of the model.

# When Calculating Gradients
- to update model, we need to multiply step-size to gradient
	- the learning rate is step-size

- small network: 2 input, 4 neurons, 1 layer, 2 output neurons
	- will reach its limit on hard datasets with unclear decision boundary
    - **It may not have enough capacity to learn complex relationships in the data.**
- increased size network: 2 input, 3 layers: {6 neurons, 5 neurons, 4 neurons}, 2 output neurons
    - **A larger network has more parameters (weights and biases), giving it a greater capacity to learn complex patterns and non-linear relationships in the data, leading to better performance on challenging datasets.**
