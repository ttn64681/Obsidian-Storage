```table-of-contents
```
- how to generate loss function
# Classification Problem
- forgot neural network
- classification is to determine if an object is member of a set or not
	- we use image classification as an example

[airplane]
- can be represented by pixel rgb values
	- (12,23,44)(...)(...) etc.etc.
- Images are represented as 3D arrays of numbers, w/ integers b/w [0,255].
	- For ex: 256x256x3

- naive way:
	- do pixel-by-pixel comparison w/ learned examples
	- L1 distance: L1 = SUM_p | I1,p - I2,p | 
	- has many disadvantaged


- Challenges: Objects may have diff size, shape, color, view angle, background, etc.
	- birds can also be in the sky

## Data-driven Approach (Naive)
- Show the system many diff examples (training set)
	- Memorize multiple templates
	- Compare test images w/ all templates

## Multi-class Classifier (Naive)
- With naive data-driven compare-based approach
- Looking for the nearest neighbor
	- can achieve 90% via direct comparison
	- but as soon as you move to more complex problem, it doesn't work

## CIFAR-10 dataset
- Labeled subset of the 80 million tiny images
	- 60,000 32x32 color images in 10 classes
	- 50,000 training images and 10,000 test images
	- Divided into 5 training batches w/ 10,000 images

- will be using this CIFAR-10 dataset for image classification

- L1 and L2 distance

## Linear Classifier:
- K-nearest neighbor classification is never used in real systems
	- terrible performance
	- Consider all pixels in the whole iamge equally important is intuitively not correct
- Parametric approach
- [plane_img] ---f(x,W)---> 10 numbers indicating score of classes
	- x: 3D array of [32x32x3] of real numbers



## Cat, Airplane, and Horse

weight matrix * input vector \* bias vector = cat, airplane, horse vector
- how to convert score to probability? -> **Softmax Classifier**

## Softmax Classifier
- The classes are exclusive to each other, only one of them could be the right answer
	- The score of class k should equal to the probability of being in this class
	- The probabilities of being in different classes should sum up to 1
	P(Y = k|X = x ) =    e^sk / SUM_j e^sk   where s = f(x,W)

Softmax output layer (vector) -> [softmax activation function] -> probabilities (vector)
- - **Ground-truth label**Â (also called one-hot encoded vector) should be like: [0, 1, 0, 0, 0]

## Loss Function
	P(Y = k|X = x ) =    e^sk / SUM_j e^sk   where s = f(x,W)

- Use the negative log likelihood of the correct class as the loss function - Negative Log Likelihood (NLL) Loss
	Li = -logP(Y = yi|X - X=xi) = -log(e^syi / SUM_j e^sj)

- Correct prediction w/ high confident:
	- syi - very large
	- **e^syi / SUM_j e^sj -> close to 1**
	- **log of that val -> close to 0**
		- this means no discrepancy
	- if **e^syi / SUM_j e^sj -> close to 0**
	- **log of that val -> close to -INF**
		- this means lots of loss

	- meaning of e^syi  in softmax activation function is

- Cross Entropy Loss:
	- H = - SUM_x p(x)log[q(x)]
	- q(x) <-> e^sx 
