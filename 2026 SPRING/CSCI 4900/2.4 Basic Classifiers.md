```table-of-contents
```
- how to generate loss function
# Classification Problem
- forget neural network
- classification is to determine if an object is member of a set or not
	- we use image classification as an example

[airplane]
- can be represented by pixel rgb values
	- (12,23,44)(...)(...) etc.etc.
- Images are represented as 3D arrays of numbers, w/ integers b/w [0,255].
	- For ex: 256x256x3

- naive way:
	- do pixel-by-pixel comparison w/ learned examples
	- L1 distance: L1 = SUM_p | I1,p - I2,p | 
	- has many disadvantaged


- Challenges: Objects may have diff size, shape, color, view angle, background, etc.
	- birds can also be in the sky

## Data-driven Approach (Naive)
- Show the system many diff examples (training set)
	- Memorize multiple templates
	- Compare test images w/ all templates

## Multi-class Classifier (Naive)
- With naive data-driven compare-based approach
- Looking for the nearest neighbor
	- can achieve 90% via direct comparison
	- but as soon as you move to more complex problem, it doesn't work

## CIFAR-10 dataset
- Labeled subset of the 80 million tiny images
	- 60,000 32x32 color images in 10 classes
	- 50,000 training images and 10,000 test images
	- Divided into 5 training batches w/ 10,000 images

- will be using this CIFAR-10 dataset for image classification

- **L1 and L2 distances are metrics used to compare image similarity.**

## Linear Classifier:
- K-nearest neighbor classification is never used in real systems
	- terrible performance
	- Consider all pixels in the whole iamge equally important is intuitively not correct
- Parametric approach
- [plane_img] ---f(x,W)---> 10 numbers indicating score of classes
	- x: 3D array of [32x32x3] of real numbers



## Cat, Airplane, and Horse
![[2.4 Basic Classifiers.png]]
**score_vector = (weight_matrix * input_vector) + bias_vector** = cat, airplane, horse vector
- how to convert score to probability? -> **Softmax Classifier**

- weights specify the importance of pixels in classifying the image into each category

## Softmax Classifier
- The classes are exclusive to each other, only one of them could be the right answer
	- **The softmax output for class k represents the probability of being in that class.**
	- The probabilities of being in different classes should sum up to 1
	P(Y = k|X = x ) =    e^sk / SUM_j e^sk   where s = f(x,W)
	![[softmax.png]]

Softmax output layer (vector) -> [softmax activation function] -> probabilities (vector)
- - **Ground-truth label** (also called one-hot encoded vector) should be like: [0, 1, 0, 0, 0]

- **do we need this during inference?** 
	- with cnn in our phones, do we always have to convert output score to probabilities? no
	- we do it to help us with training
	- otherwise we can directly use the scores to make decisions
	- <span style="background:#fff88f">it's actually preferred during inference, because it costs more time and power/memory</span>
	

## Interpreting a Linear Classifier
![[linear classifier.png]]

## Loss Function
	P(Y = k|X = x ) =    e^sk / SUM_j e^sk   where s = f(x,W)
	- SUM_j e^sk: sum of all the e^sj parts

- Use the negative log likelihood of the correct class as the loss function - **Negative Log Likelihood (NLL) Loss**
	Li = -logP(Y = yi|X - X=xi) = -log(e^syi / SUM_j e^sj)

- Correct prediction w/ high confident:
	- syi - very large
	- if **e^syi / SUM_j e^sj -> close to 1**
		- this means high confidence
	- **log of that val -> close to 0**
		- this means no discrepancy
	- if **e^syi / SUM_j e^sj -> close to 0**
		- this means low confidence
	- **log of that val -> close to -INF**
		- this means lots of loss

	- meaning of e^syi  in softmax activation function is

- Cross Entropy Loss:
	- H = - SUM_x p(x)log[q(x)]
	- q(x) <-> e^sx 
		- **q(x) here represents the predicted probability from the softmax output.**


- for diff tasks, we use diff loss functions
- e.g.
	- for regression task, we use **squared mean error loss** to identify discrepancy b/w result and ground-truth label
	- for classification task, we use **negative log likelihood loss** 

## Loss Function and Overfitting
- loss function quantifies the quality of a model
- **used during the training to guide the learning of the weight**
- *Potential Problem of Training: overfitting*
![[overfitting.png]]

- Can be due to:
	- Huge network: too many params, or too many weights
	- Insufficient Training Data
	- Poor Data Quality
	- Excessive Training Time
		- Model will learn generalized pattern from training data first
		  - then, it will learn the details over time, and this can lead to overfitting

- **Weight regularization encourages models to have smaller, less complex weights. This leads to smoother decision boundaries, which generally means the "function curve" (representing the model's output) is less "wiggly" and thus generalizes better to unseen data.**
- i.e. the model tries to fit not only the regular relation between inputs and outputs, but also the sampling errors

## Weight Regularization
![[weight regularization.png]]
- - If weights have a larger magnitude, the model can fit the training data more closely, potentially resulting in **less _training prediction error_** (lower data loss). However, regularization _adds a penalty_ for these larger magnitudes, increasing the _total loss_ to encourage simpler models and prevent overfitting.
- function curve becomes more steep (skinner), which can cause overfitting
- if we make function curve flatter, it generalizes more

# Linear Classifier
## Learn the Linear Classifier
- loss curve would look more complex
- to simplify we just say it looks like this:
![[Learning Linear classifier.png]]

- to make the model go right, we must increase the Weight
- to make the model go left, we must decrease the Weight

- after we find direction to change weight, we don't know how much we should increase or decrease the weight

"We need multiple iterations to learn the model because the process of finding the best parameters (weights) is iterative"

## Gradient Evaluation
- Analytical gradient evaluation is implemented/calculated using back propagation
	- Inference involves forward propagation only
	- Learning involves both forward and backward propagation
![[Gradient Evaluation.png]]
![[Lsoftmax - penalty.png]]

- In simple terms, **`lambda R(W)` is a "complexity penalty"**:
	- It's a part of the total cost that punishes the model if its internal `W` (weights) become too large or complex.
	- Its purpose is to prevent the model from overfitting the training data by making it simpler and more generalizable.

## Gradient Descent
- f(x,W,b) = Wx + b = y
![[Function Curve and Error Surface.png]]
- the optimal solution is called the Global minima on the loss curve

Imagine the "loss curve" or "error surface" as a mountainous landscape where the height represents the `Total Loss`. Your model's weights (`W` and `b`) determine its position on this landscape.

- **Global Minimum:** This is the absolute lowest point in the entire landscape. If your model's weights correspond to this point, it means your model has found the best possible set of parameters that minimize the total loss. This is your ultimate goal.
- **Local Minimum:** These are "valleys" or "dips" in the landscape that are lower than their immediate surroundings, but _not_ the absolute lowest point overall. An optimization algorithm like gradient descent can get "stuck" in a local minimum if it doesn't have enough momentum or a large enough step to climb out of it and find a lower valley.
- **Candidate Solution:** At any point during training, the current set of weights (`W` and `b`) represents a "candidate solution." The goal of training is to iteratively refine this candidate solution to move closer to the global minimum.


- at any point, we don't know if we're at global or local minima, so we can just compare the minimas to other minimas

## OPTIMIZERS 
- **Different types of optimizers**
	- SGD (Momentum)
	- AdaGrad
	- RMSProp
	- Adam