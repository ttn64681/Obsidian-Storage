```table-of-contents
```
# Review
Last week:
- intro to convolutional neural networks
	- convolution layer
		- filter size w/ multiple channels
	- number of output channels == number of filters

- Stride in Convolutional Layer
	- Output size?
		- count pixels at center of filter, row and col wise to get MxN
		- *(N-F) / stride + 1*
		- problem: big filter -> output might be smaller than desired
		- solution: zero padding

- the receptive fill is the area that a filter can capture at once
- **disadvantage:**
	- larger filter = higher computation cost
		- why? have to slide it across each pixel, doing element-wise multiplication each time

# Size Calculation
- Input vol: 32x32x3, 10 filters: 5x5x3
- stride = 1
- pad = 2pxls

- **what is output vol size?**
	- 32 x 32 x 10
- **what is num of params in this layer?**
	- *how many weights* x *how many filters* 
	- = ?
	- *for each filter, how many weights?* 
		- 5 x 5 x 3 = **75-dimensions**
	- = 75 x 10 = **750 weights**
- **what is computation cost?**
	- *how many multiplications to generate 1 output pixel?*
		- 75 **MACs** 
	- *how many multiplications to generate output layer (32 x 32 x 10)?*
	- = (5 x 5 x 3 ) * (32 x 32 x 10) = **768,000**
	- (not including bias as parameters)

- what is size of FCL? (e.g. 10 x 20)
	- what is num multiplications?
		- input x weight
		- 10 x 20

## Summary of ConvLayer
- Input Volume: W1xH1xD1
- Requires 4 hyperparams:
	1. \# filters: **K**
	2. filter size: **FxF**
	3. stride: **S**
	4. \# zero paddings: **P**
1. Output vol: W2 x H2 x D2

# Pooling Layer
- **used to reduce feature map size**
	- if we have huge feature map, we have to do sliding window operations over huge map!

- pooling layer consolidate the nearby features and reduces feature map
- operates on diff feature maps independently
## Max Pooling
- **Max pooling:**
	1. determine stretch val (standard is 2)
	2. divide filter size by stretch size
	3. **take the max val of each quadrant**
	- Ex:
		- 4x4 filters -> (split into four 2x2 quadrants, pick only max val of each)
		- -> 2x2 filters, stride = 2

	- *For 3x3 (odd number size), you pad with zeros to get 4x4*

- For updating parameters via backpropagation, max pooling = No computation cost!
	- this is because there are no weights/biases associated with this process

## Average Pooling
- **Average Pooling:**
	1. same process as Max Pooling, but **take average of quadrant instead of max**

### Why use Convolutional Layer if Pooling Layers are lightweight?
- goal is not shrinked size
- goal is to extract the features
	- pooling layers cannot do this, b/c no learnable params (weights)
		- no computation
	- convolutional has learnable weights/params to extract features to train upon

### Why use Pooling Layers at all?
- for later computation cost saving
	- if computation cost wasn't a problem, we wouldn't use Max Pooling

## What about backdrop and gradient?


## Summary of Pooling Layer
- Requires 2 hyperparameters
	- Filter size: F
	- Stride: S
- No weights nor bias
	- no trainable params


# Fully Connected Layer
Ex:

| conv | ReLU | conv | ReLU | Pooling | conv | ReLU | conv | ReLU | Pooling | conv | ReLU | conv | ReLU | Pooling | FC  |
| ---- | ---- | ---- | ---- | ------- | ---- | ---- | ---- | ---- | ------- | ---- | ---- | ---- | ---- | ------- | --- |

**Backpropagation model:**
- one neuron gets small region
	- simple cells
- the higher level cells contains more regions, since it contains simple cells
	- complex cells
- the highest/deepest layer contains global information

# Matrix Implementation
- **General Matrix Multiplications**
- 3 input channels -> each filter will have 3 channels

- (matrix product version of convolution for capturing each sliding window)
- input features --convolution(sliding window 1)-->
- -> one output pixel
- ->output features

- **GMMs are fundamentally an algorithmic and statistical modeling choice.** Their utility comes from their ability to model complex distributions probabilistically
### Why use GMM's despite memory intensive computation?
1. **Generative Capabilities:** GMMs are generative models, meaning they can be used to generate new data samples that resemble the original data distribution.

- So, the memory overhead and computational cost are genuine concerns, but 
- the modeling flexibility and probabilistic nature of GMMs often outweigh these issues, and 
- GPUs can help manage the computational aspect.
# LeNet: C1
- 32x32 Input
- C1: feature maps 28x28
- S2: 14x14 subsampling layer
	- this is just pooling layer
- C3: 10x0
- S4: 5x5
- C5: 120
- F6: 85
- Output: 10 FCL

- Convolution layer is for extracting the patterns
- later layers are for flattening

- this neural network can easily achieve 95% accuracy

## Additional Info
- first use of ReLU
- norm layers are not common anymore
- heavy data augmentation
- 
### Dropout (review)
- dropout intermediary results during training to help mitigate overfitting problem

# VGGNet
- ConvNet Configuration:
	- have a 11 weight layer version
	- a 13 weight layer version
	- a 16 weight layer version
	- a 19 weight layer version

		- each w/ inp size (224 x 224 RGB image)
		- but each w/ increasing # conv layers => increased depth of network

	- plain network
	- "keep it deep. keep it simple"

	- **Key characteristics:**
	    - Uses very small 3x3 convolutional filters throughout.
	    - Stacks these small filters consecutively to achieve larger effective receptive fields.
	    - Uses max pooling to reduce spatial dimensions.
	    - Ends with fully connected layers for classification.

# GoogLeNet
- getting larger receptive fill by having larger kernel size
- Filter concatenation

- automatically choose diff type of kernels for diff image region.
- (Mix & match and not increase the output size)

# Conv + Pooling + FC
- Flow:
	- convolution + ReLU
	- max pooling
	- full connected + ReLU

- how many weights do we have?
	- FC1: 7 x 7 x 512   x   4096 = 102,760,448 (103M)
		- input x weight
	- FC2: 4096 x 4096 = 16,777,216 (17M)
		- 4096 = num input neuron
		- 4096 = num output neurons
	- FC3: 4096 x 1000 = 4.1M

### FCL vs ConvLayer: Which has greater computation cost?
- **FCL**
- *Why?*
	- every neuron in 1 layer connects to every neuron in next layer.
	- this means N * M weights (M is output neuron size)
	- this number becomes astronomical for large input dimensions
		- e.g. flattened feat maps from deep conv layers

### What are problems of using these type of arch design?

- **Model size is huge:**
    - The Fully Connected Layers (FCLs) at the end of the network often require a massive number of weights to connect all the extracted features. This is because every neuron in one layer connects to every neuron in the next. For large input dimensions (e.g., from deep convolutional layers), this leads to a very large model, consuming significant storage and memory.
- **Cannot easily use arbitrary input sizes:**
    - The architecture's input size is often fixed due to the FCLs.
    - **Why?** FCLs require a specific number of input neurons, which is determined by the flattened output of the preceding layer (often convolutional or pooling layers).
        - If you change the input image size (e.g., from 32x32 to 64x64), the number of "feature pixels" after convolutions and pooling will change.
        - The FCL is designed to accept a fixed number of inputs. If the flattened output size doesn't match the FCL's expected input size, the network cannot process the data correctly. It's like trying to plug a square peg into a round hole.
# Test
- Input vol: 32x32x3, 10 filters: 5x5x3
- stride = 1
- pad = 2pxls

- **what is output vol size?**
	- 32 x 32 x 10
- **what is num of params in this layer?**
	- *how many weights* x *how many filters* 
	- = ?
	- for each filter, how many weights? 
		- 5 x 5 x 3 = 75-dimensions
	- = 75 x 10 = 750 weights