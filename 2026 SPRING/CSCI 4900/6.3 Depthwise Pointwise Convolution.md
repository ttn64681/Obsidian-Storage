```table-of-contents
```
# Depthwise
- does 1 channel at a time

# Pointwise
- same as conventional convolution, just 1x1

# Reducing Internal Covariate Shift
- with internal covariate shift, model cannot focus when distribution is constantly changing
- making it a gaussian, more centered ditribution, you reduce gradient explosion and vanishing
- this is goal of batch noramlization
- after batch normalization, internal covariate ....

after batch normalization, we need to scale and shift gamma and beta 
train gamma and beta to scale and shift distribution to let data represent a unique feature
preserve network to have feature extraction capability

## Review: 4 parameters
- gamma, beta - trainable (scale and shift)
- mean and variable - not trainable (statistical purposes: ...)


- when training model, we want global mean to better represent overall distribution
- every batch that's,  we will add the batch mean to ....
- global mean and variance ....


- thats why we need to set up model train mode and moedl evaluation mode
- model evaluation - models will not be trained anymore
- training - dropout layer
- inference - droupout layer doesn't work

# Batch Normalization
- each channel has its own mean and deviation
- each channel gives .....
- if you have 6 channels, then you will have 64 ... in batch normalization


## Benefits of Batch Normalization
- Inclusion of Batch Norm in DNNs improves training time
- BN enables utilization of larger learning rates, this shortens the time of convergence when training neural networks
- sometimes your model won't converge at all, which makes model unstable... this because.... when.....
- batch norm prevents this, reduces variance...
- reduces common problem of vanishing gradients
- covariate shift within neural network is reduced

# Batch Normalization in Inference
- mini Batch mean (fixed)
- mini-batch variance (fixed)
- this gives advantages, sicne you don't want to train them

## Disadvantages of using Batch Normalization Layer
- have to add another physical layer between ConvLayer and Activation Layer, for EVERY ConvLayer
	- you are adding AMANY layers to network
	- this means:
		- more mem consumption
		- more mem cost to store intermediate results
		- more mem access time -> slower inference speed
			- latency


# Speed up inference w/ BN Folding
- you unfold batch norm into 2 sep layers, merging them into one.... ?
- convolution op followed by batch norm op can be expressed for an input x as:
```
z = Wx + b
out = sigma * z-u /
		sqrt(gamma**2 + epsilon)



```
We can re-arrange the W and b of the conv to take the params of batch norm into account, as such:
```
w_fold = sigma * W /
		sqrt(gamma**2 + epsilon)
b_fold = b -u /
		sqrt(gamma**2 + epsilon)


```

can only fold during inference, not training
- you get global mean of batch from training. so you can only fold after, during inference

# speed-up inference w/ BN Folding
- the inference time for a single image is:

- VGG16 w/ BN
		- Without BN folding:
			- 2.79 ms
		- With BN folding:
			- 2.4 ms

- ResNet50 w/ BN
	- Without BN folding:
		- 6.17
	- With BN golding:
		- 4.67ms
	- saved ~24 inference time, 1.32X accel!

# HW1
- measure network inference time
	- to do this:
		- use 1 single image to measure inference time
		- don't use training batch
		- don't measure time of training batch
		- run multiple loops, and calc mean and variance of results
			- 100 loops, 1000 loops
		- Usually, we also need to run several warmup rounds before profiling the inference time!
			- fill up cache
			- warm up memory

# Speed-up inference w/ BN Folding
- need to decide where to insert bn layer, and whether the bn layer can be folded or not

- besides batch normalization, there are many other types of normalization layers
# All types of Normalization

- Layer Norm
	- calc norm across each channel, and even across. batch dimension
- Instance Norm
- Group Norm

N - stands for Batch
C - represents Channel
H-W - represents Height and Width


batch norm -
- same channel, calc mean of batch

layer norm -
- C x H, W -> this is pixels for all images across all channels, but ...
- normalizes based on its OWN values

instance norm -
- pixels of same image from same channel ...
- tradeoff

all types of normalization - 
- Batch Normalization
	- calc mean and std (same for all training examples)
- Layer Norm
	- calc mean and std (same for all feature dimensions)

### Can we fold layer norm?
- NO
- we norm data based on it's OWN value in layer norm, so no way to get global mean and variance in advance

- inputs are changing
- we couldn't put this value into weights, because weights are FIXED

- REMEMBER
- we can do batch norm, very importnat, everyone doing this doing inference
- but we canNOT fuse the layer norm

- in LLM, they are using layer norm or group norm, not batch norm, so are NOT able to take advantage of fusion technique



# Midterm exam:
#test 
- after spring break

- Exercise that will be tested:

- assume we know that output feature map size 5x5
- can we calc computational costs of general conv and depthwise separable conv?
- what else do we need to know?
- kernal size 3x3
- what are num of params and computation costs?

1. need to know how many params, to calc computation cost
2. how many filters? 5x5x32 output layer (input XxXx16, 16 is channels for input)
	1. each filter is 3x3x16, since it needs to get all channels from input
3. num channels of each fulter


computation of 1pxl by 1 filter:
- 3x3x16
so total of entire output layer is = 
- **3x3x16 \* 5x5x32**
= **115,200** (MACs?)

what is num of weights?
- 3x3x16 (kernal size * input channels) * filter amount , which is output channels

params: 3x3x16

## for depthwise convolution
- computation cost of 1 pxl:
	- **3x3,** since it doesn't go across all layers (only on 1 channel)
- total computation of all pixels of output:
	- **3x3 \* 5x5x16**
= 3.6K

- num of params?
	- do we need more info like if stride is 1?
	- 

1x1x16x32

## for pointwise
- computation cost of 1 pixel
	- 1x1x16, since one point but through all channels
- total:
	- **1x1x16 \* 5x5x32**
= 16.4K
# Test
- Input vol: 32x32x3, 10 filters: 5x5x3
- stride = 1
- pad = 2pxls

- **what is output vol size?**
	- 32 x 32 x 10
- **what is num of params in this layer?**
	- *how many weights* x *how many filters* 
	- = ?
	- for each filter, how many weights? 
		- 5 x 5 x 3 = 75-dimensions
	- = 75 x 10 = 750 weights