```table-of-contents
```
# Review

- ∂L/**∂w1_i,j** = ∂L/x_j \* ∂x_j/∂w1_i,j 

- the relationship of ∂x/∂w is = s1\*w
	- local gradient of any weight = input \* upper level gradient

- after calculating gradient of each weight:
	- update 
		- wi = wi - step_size \* ∂L/∂wi
	- subtract step_size b/c we want to go opposite of gradient
		- greater W =
		- lesser W = 

## Forward Propagation
- inference propagation

- start at

## Backward Propagation

- start at Loss
- Calc Gradient activation
- Calc Gradient of weight

# Vectorization
- neural network doesn't calc every individual weight, it calculates the vector
	- ∂Y = [∂L/∂y1, ..., ∂y_D]

- so far we haven't considered bias and activation function
- if we consider the activation function,  it's same thing:

## Backward Propagation w/ Activation Function
- L = (a-t)^2
- ∂L/∂z = ∂L/∂a \* ∂a/∂z
- ∂L/∂a = 2(a-t)

- it just adds one more term


- when calculating gradient of this, you calc gradient of each part
- ∂a/∂z - is more complicated, and so is calculating its derivative
	- Sigmoid, Softmax, Tanh, ReLU
	- it's okay though, just remembering it is fine

- each activationg function has its own format of the derivative

# Derivation of Activation Functions
## ReLU
- f(x) = max(x,0)
- **∂f(x)/∂x =**
	**{ 0           x<0**
	**{ 1            x>0**
	**{ None  x=0**

- During training, we can assign either 1 or 0 as the derivative of ReLU **at x=0**



- s0 if you use ReLU function here, the derivative of ∂a∂z is either 0 or 1
	- depends on what z (input) you have
	- if z = -2, then derivative = 0
- in this way you can calculate 

- **so, in this way you discard negative information, but sometimes negative info is IMPORTANT**
## Leaky ReLU
- *so, in this way you discard negative information, but sometimes negative info is IMPORTANT*
- f(x) = max(0.01x, x)
- **∂f(x)/∂x =**
- 	**{ 0.01     x<0**
- 	**{ 1            x>0**
- 	**{ None  x=0**


## Sigmoid
- f(x) = 1/1+e^-x
= 
## Tanh
- f(x) = e^x - e^x / e^x + e^x
- = 1 - f(x)^2

## Softmax
- also hard to calculate, but luckily you don't need to

# Computation costs
- back prop generally costs 2X computation than Forward
- So, the total computation costs of training is 3X of inference
	- Training: FW Prop + BW Prop (=3)
	- Inference: only FW Prop (=1)

- 1 MACs = 2FLOPs
- B/c 1 MAC includes a multiplication and an accumulation operation, while each multiplication and accumulation counts for 1 Floating-Point Operation separately




```table-of-contents```