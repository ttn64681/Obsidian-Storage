```table-of-contents
```

How to train?
- training data
	- sent to neural network
	- need network to predict results
How to tune?
- need **loss function** and **regularization function**
	- loss function: discrepancy b/w predicted and ground-truth results
		- based on the calculations, we get the **loss**
		- based on the loss, we see **how weight changes affect loss**
	- **final goal:** we want to make loss smaller
		- how to determine which direction to change our weights?
			- based on the gradient
			- calculate gradient of weight with respect to loss function
			- we calc gradient at this point on the loss function
				- if pos slope, we move left
				- if neg slope, we move right
				- we make a small step in that direction
				- we then *recalculate the gradient at the new point*
					- **that's why we need multiple iterations**
	- we need model to get to global minima, not local minima
	- can't say that we are at the global minima! we don't know that!
	- all we can say is "this model is better than this other one"

- **Overfitting** means:
	- model fits the training data TOO well
	- it memorizes training data, but does not work well on unseen data
- Overfitting is caused by:
	- poor data quality:
		- not only badly labeled data, but also 
	- too complex of neural network:
		- especially when used on too easy of a problem
	- 

![[Drawing 2026-01-28 16.44.21.excalidraw]]

- Solution: Weight Regularization
	- flatten curve to minimize overfitting

# Calculate Gradient
- **slope** = y2-y1 / x2-x1 
- Ex: f(x) = x^2 
	- we can caluclate f(x + Δx):
		- f(x + Δx) = (x + Δx)^2
		- Expand (x + Δx)^2:
			- f(x + Δx) = x^2 + 2x Δx + (Δx^2)
	- Slope formula is: ( f(x + Δx) - f(x) ) / Δx
	- Put in f(x+Δx)
		- x^2 + 2x Δx + (Δx^2) - f(x) / Δx

- **derivative:** tells us the slope of a function at any point

## Practice Derivatives
y=e^-2x
y' = -2e^-2x

y=(2-x)^2
y'= -2(2-x)

sigmoid function: activation function
- tanh
- relu
σ(x) = 1 / 1+e^-x
σ'(x) = (1+e^-x)^-1
σ'(x) = -1(1+e^-x)^-2  (-e^-x)
σ'(x) =  -(e^-x) / -(1+e^-x)^2 // cancel negatives in numer & denom
σ'(x) =  **(e^-x) / (1+e^-x)^2**

Calculate derivatives of each of parameters inside neural network
## Calculate partial derivative
- Computation Graph
- if fn involves multiple vars (e.g. f(x,y,z)) and we are interested in how fn changes w/ respect to one specific var (e.g., x), we use partial derivatives
![[Drawing 2026-01-28 17.16.17.excalidraw]]

- **Ex: f(x,y,z) = (x+y)z**
	- e.g., x = -2, y = 5, z = -4
	- ∂f/∂x =?
	- ∂f/∂y =?
	- ∂f/∂z =?

**How to find derivates:**
∂f/∂z =?
1. find relation b/w **f(t0p) and z(bot)**
2. <span style="background:#fff88f">f = q * z</span> <-- // z(bot) is variable, so q is constant
3. f' = (q\*z)' = q 
- ∂f/∂z = *q*

∂f/∂x =?
1. find relation b/w **x and f**
2. *chain rule :* 
	1. *∂f/∂x = ∂q/∂x \* ∂f/∂q*
		1. ∂f/∂q = (q \* z)' = **z**
		2. ∂q/∂x = (x+y)' = **y** 
	2. = **zy**
- ∂f/∂x =*zy*

∂f/∂y =?
1. find relation b/w f and y



we can reuse these long chains in neural network:
- when moving backwards a layer, back-propagation, we don't need to recalculate all the vals in the upper-layer 
- we just reuse previously computed results during back propagation


- local gradient involves the partial derivative function we care about finding currently,

## Remember the point:
- The goal of this is to find the weight (w) in order to minimize loss (L)

## Application: Backward Propagation
