```table-of-contents
```
# Intro
- Focus on linear systems and determinants
- Links solutions of linear systems to intersections of lines or planes

- efficient desc of large data sets using feature vectors
- Constructing low-dimensional approximations of the available data thru projection and least squares

## Why Linear Algebra
- Under the hood of every algorithm:
	- **Regression:** Minimizing squared errors (Linear Algebra)
		- modeling relationship to **predict value of dependent var based on independent**
	- **PCA:** Finding principal axes (Linear Algebra)
		- simplifies complex datasets by transforming them into a **lower-dimensional space**, which can help in *visualization, noise reduction, and improving the efficiency of machine learning algorithms*
	- **Neural Networks:** Matrix multiplication (Linear Algebra)

- **NumPy / PyTorch / TensorFlow handle the math**

### 3 Fundamental Metrics
- *Scalar*
	- single number (x in R)
	- Ex: Learning rate, bias term
- *Vector (1-D Tensor):*
	- **an Array or list of attributes**
	- Ordered list of nums (x in R^n)
	- Ex: Single row in database
- *Matrix (2-D Tensor):*
	- A 2D grid of nums (A in R^(mxn))
	- Ex: An Excel sheet (Samples x Features)
		- ***samples = x***
		- ***features = y***

![[Drawing 2026-01-20 13.24.50.excalidraw]]

### High-Dimensional Metrics
- Tensor:
	- Matrices of N-dimensions
	- **Rank-3 Tensor:** Time series data
		(Rows x Cols x Time)
	- **Rank-4 Tensor:** Image Batch
		(Batch Size x Height x Width x Color)
		- Each color/pixel is split into r,g,b

# Vectors:
- an array or list of attributes

## Data point:
- represented using vectors or collection of vectors (matrix)
- Ex:
	- A "House" vector: [2000,3,500]
	- 2,000 sq. ft, 3 beds, $500k

-  a collection of vectors is matrix

## Vector operations:
- Vector Addition
- Vector Subtraction
- Scalar Multiplication

### Dot Product:
- math def: 
	- a x b
	- helpful for vector multiplication
- geometric def:
	- ->a x ->b = |->a| |->b| cosO

### Cosine Similarity
- tells us which direction 
- Range: -1 (Opposite) to 1 (Identical)
	- 0 = Perpendicular
	- helpful for recommendation systems

## Vector Norm
- A function that measures the **size** or **magnitude** of a vector by *quantifying its length from the origin*
- The vector norm |x|\_p for p = 1,2,... is defined as:
	- |x|\_p = 
- Properties:
	- non-neg |x| > 0 when x≠0
	- Definite |x| = 0 iff x = 0
	- Triangle ineq |x+y| ≤ |x| + |y|

- L1-norm: p=1
	- dist b/w two city blocks
- L2-norm: p=2
	- euclidean distance
- L3-norm
- L4-norm
- L(inf)-norm

## Transformation
- y = Ax
- Input: Vector x
- Function: Matrix A
- Output: Vector y

- Matrices only perform Linear Transformations:
	- Rotate, Scale, Shear, Reflect

Indentity
Reflection
Translation
Scale
Rotation
Shear-X
Shear-Y

## Matrix Multiplication
- Inner dimensions must match
- (mxn) x (nxp) = (m x p)
- Application - 
	- Neural Networks: a chain of matrix multiplications
	- Layer\_2(Layer1( )) 

- **Identity Matrix (I)**
	- Square matrix w/ 1s on diagonal, 0s elsewhere
	- **A x I = A**
- **Inverse MAtrix (A^-1)**
	- **A x A^-1 = I**
	- A square matrix that does not have a matrix inverse is a Singular Matrix

## Solving Systems of Equations
- The problem: Ax = b
- A: Data/Features
- x: Weights (Unknown)
- b: Targets/Labels 

- Solution: x = A^-1 b
- Inverting large matrices is slow/unstable
- Use approximation algorithm (Gradient Descent)


# Quiz


# Notes