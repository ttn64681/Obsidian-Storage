```table-of-contents
```

# Model Uncertainty
## Deterministic vs Stochastic Systems
**Deterministic**
- **Zero randomness; outcomes are fixed**
- Same inputs always yield the same output
- 100% predictable if initial conditions known
- Simple, rigid and efficient
**Stochastic**
- **Inherent randomness or uncertainty**
- Same inputs yield a distribution of possible outcomes
- Only probabilistic predictions
	- (e.g., averages or likelihoods)
- Complex, statistical frameworks

## Model Uncertainty
- In deterministic systems, uncertainty is a practical limitation
	- e.g. position of object given initial velocity and position
- **In stochastic systems, uncertainty is a built-in feature of reality**
	- e.g. population of country in 5 yrs

# Statistical Learning
![[statistical learning models ex.png]]
- X - independent variables (predictors)
- Y - dependent variables (response)

- We assume there is some relationship b/w Y and X = (X1,X2,...,Xp), which can be written in the general form
	- **Y = f(X) + Îµ**
		- f(X) - estimation function given input X
		- Îµ - error term
- In essence statistical learning refers to a set of approaches for estimating f

## Why estimate f?
- in many situations, a set of inputs X are available, but output Y cannot be easily obtained
	- ^Y = ^f(X)
	- **(Predicted response) = (estimation function given X)**
- Reducible and irreducible error
	- Reducible - error due to imperfections of model
		- you can reduce error by choosing better model, improving estimation technique, or getting better/more data
	- Irreducible - error that cannot be eliminated no matter what.
		- comes from noise or randomness (Îµ) in data itself
		- even with the true f(X), can't predict Y perfectly

# Probability
- The uncertain phenomenon of interest
- P(event) = times event occurs / total repetitions

- given sample space **Omega**, let C be collection of events, a probability measure P is a function, which maps events in C to a number b/w 0 and 1, satisfying the following axioms:
	- All probabilities are nonnegative, P(A) â‰¥ 0 for any event A in C
	- Probability of sample space is one, P(**Omega**) = 1
	- If events A1, A2, ... , An in C are disjoint, then prob of their union equals sum of their indiv probabilities

## Random Var
- a measurable fn X:Omega -> E mapping the sample space to a measurable space
- *function that maps an outcome from the sample space (`w`) to a real number.*
	- Consider a probability space representing 2 rolls of a 6-sided die
	- Each possible outcome can be encoded as a vector w/ 2 entries
```
w:= [w1],      w1,w2 in {1,2,3,4,5,6}
    [w2]
    
w1 = outcome 1st roll
w2 = outcome 2nd rolls
```
1. Modeling result of first roll:
	- ~a(w) := w1
		- ~a - random var that maps sample space to real number
		- this function seems to give the result of the first roll
	- the range of ~a is {1,2,3,4,5,6}
```
For ~a = 1:
	{w:~a(w) = 1} := {[1],[1],[1],[1],[1],[1]}
					 {[1],[2],[3],[4],[5],[6]}
```

# Calculating diff types of Random Vars
## Probability Mass Function (PMF)
- **Meaning:**Â ForÂ **discrete**Â random variables (like a die roll, where outcomes are distinct, countable numbers), the <span style="background:#fff88f">PMF gives the exact probability for each possible outcome.</span>
- **Point:**Â It allows you to visualize and calculate the likelihood ofÂ _each specific value_Â a discrete random variable can take. For yourÂ `~a`Â (first roll of a die), the PMF would state P(`~a` =1) = 1/6, P(`~a` =2) = 1/6, etc.

- if the ~a is a discrete variable, a mathematical function to map the probability of ai,
	- p_~a (ai) = P(~a=ai)
- Properties:
	- o â‰¤ p_~a (ai) â‰¤ 1
	- Î£p_~a (ai) = 1
![[PMF.png]]

## Probability Density Function (PDF)
- **Meaning:**Â ForÂ **continuous**Â random variables (e.g., height, temperature, time), the PDF describes theÂ _relative likelihood_Â for a random variable to take on a given value. You cannot get a direct probability for a single point, but rather for a range.
- **Point:**Â It allows you to calculate the probability that a continuous random variable falls within a certain range (by integrating the PDF over that range).
	- DOESN'T APPLY TO DICE EXAMPLE

- if the ~a is a continuous var, the function maps probability density
	- P(a-Îµ â‰¤ ~a â‰¤ a) = Integral_a-Îµ to a [f_~a(ai)]
![[PDF.png]]

## Cumulative Distribution Function (CDF)
- **Meaning:**Â For both discrete and continuous random variables, the CDF gives the cumulative probability that a random variable will be less than or equal to a certain valueÂ `a`.
- **Point:**Â It provides a complete picture of the probability distribution, showing the probability of gettingÂ _up to_Â a certain value. For yourÂ `~a`, F_`~a`(3) would be P(`~a`Â â‰¤ 3) = P(`~a`=1) + P(`~a`=2) + P(`~a`=3) = 1/6 + 1/6 + 1/6 = 3/6 = 1/2.

- the function to map the probability of ~a â‰¤ a
	- F_~a(a) := P(~a â‰¤ a)
- Consider the following CDF:
![[CDF.png]]

## Expected Value
- **Point:**Â It represents theÂ **long-run average**Â or mean of a random variable. If you were to repeat the experiment many times, the expected value is what you'd expect the average outcome to be. For a single 6-sided die roll, E[X] = (1*1/6) + (2*1/6) + ... + (6*1/6) = 3.5.

- Weighted Average where each possible outcome is multiplied by its probability
	- Discrete variable: E[X] = SUM xi \* P(xi)
	- Continuous variable: E[X] = Integral_-INF to INF [ x \* f(x) dx]
![[Expected value 6 sided die.png]]


# Characterizing Distributions
## Moments
- quantitative measures that describe theÂ **shape and characteristics of a probability distribution**Â (or a set of data points).
- Different moments reveal different properties of the distribution.

- The n-th moment of a random variable is the expected value of its n-th power.
	- Let X be a random var and n in N
	- if Î¼_x (n) = E[X^n] exists and finite, this is n-th **moment** of X
		- 
	- ğœ‡Ì„_x (n) = E[(X - E[X])^n] is the n-th **central moment**
		- this is the spread or deviation from avg expected value
		- This is the expected value of theÂ _deviation of X from its mean_Â (E[X]), raised to the power ofÂ `n`.

## First Moment
- Mean Î¼
- Measures central tendency
- Highly sensitive to outliers

## Second Moment
-  Variance sigma^2
- Measures the dispersion from mean Î¼
- Var(X) = E[(X -Î¼)^2]
	- Î¼ is E[X]

## Third Moment
- Skewness gamma_1
- Measures the asymmetry
- gamma_1 = E[ (x-Î¼)^3 / sigma]
- Figure:
	- left-skewed (negative)
		- tells how flat the tail is
![[Left skew.png]]
## Fourth Moment
- Kurtosis K
- Measures the "tailedness"
- K = E[ (X-Î¼)^4 / sigma]
![[Kurtosis.png]]


# Notes
*   **Model Uncertainty**:
    *   **Deterministic Systems**: Zero randomness, fixed outcomes, 100% predictable.
    *   **Stochastic Systems**: Inherent randomness, outcomes are distributions, only probabilistic predictions.
    *   Uncertainty is a practical limit in deterministic, a built-in feature in stochastic.
*   **Statistical Learning**:
    *   Relationship: `Y = f(X) + Îµ` (response = estimation function + error).
    *   **Reducible Error**: Can be reduced by improving the model, estimation, or data.
    *   **Irreducible Error**: Cannot be eliminated; comes from inherent noise (`Îµ`).
*   **Probability**:
    *   `P(event) = times event occurs / total repetitions`.
    *   Axioms: Nonnegative `P(A) â‰¥ 0`, `P(Omega) = 1`, disjoint union `P(âˆªAáµ¢) = Î£P(Aáµ¢)`.
*   **Random Variable (`~a` or `X`)**:
    *   A function mapping outcomes from a sample space (`w`) to real numbers.
    *   Example: `~a(w) := w1` (maps two-roll outcome `w` to the first roll `w1`).
*   **Probability Mass Function (PMF)**:
    *   For **discrete** random variables.
    *   `p_~a(aáµ¢) = P(~a=aáµ¢)`: Gives the exact probability of each specific outcome.
    *   Properties: `0 â‰¤ p â‰¤ 1`, `Î£p = 1`.
*   **Probability Density Function (PDF)**:
    *   For **continuous** random variables.
    *   `f_~a(a)`: Describes relative likelihood; probability is found by integrating over a range.
*   **Cumulative Distribution Function (CDF)**:
    *   For both discrete and continuous variables.
    *   `F_~a(a) := P(~a â‰¤ a)`: Gives the cumulative probability of `~a` being less than or equal to `a`.
*   **Expected Value (`E[X]`)**:
    *   The **long-run average** or mean of a random variable.
    *   Weighted average: `Î£xáµ¢P(xáµ¢)` for discrete, `âˆ«x f(x) dx` for continuous.
*   **Moments**: Quantitative measures describing the **shape and characteristics** of a distribution.
    *   **n-th Moment**: `Î¼_x(n) = E[X^n]`.
    *   **n-th Central Moment**: `ğœ‡Ì„_x(n) = E[(X - E[X])^n]`. Measures deviation from the mean.
*   **First Moment (n=1)**: **Mean (Î¼)**; measures central tendency.
*   **Second Moment (n=2)**: **Variance (ÏƒÂ²)**; measures dispersion from the mean.
*   **Third Moment (n=3)**: **Skewness (Î³â‚)**; measures asymmetry.
*   **Fourth Moment (n=4)**: **Kurtosis (K)**; measures "tailedness" (peakedness/heavy tails).

**In essence**: You learn about Model Uncertainty to understandÂ _why_Â we need these tools. Then, Probability and Random Variables provide theÂ _framework_Â to represent uncertain events numerically. PMF/PDF/CDF are theÂ _detailed maps_Â of how probabilities are distributed. Finally, Expected Value and Moments are theÂ _summary statistics_Â that help you quickly grasp the center, spread, and shape of these probability distributions, which is vital for analysis and prediction in statistical learning