```table-of-contents
```

# Intro


μ - sample mean of data
## Central Limit Theorem (CLT)
- **n increase, sample means distrib --> normal distrib (bell-shape)**
- sampling distrib of sample means approaches a normal distrib as sample size (n )gets larger
- Independent of distrib of population

# Variance vs Covariance
- Variance measures variation (spread) of single random var
	- random var - could be a feature
	- variance shows us how far it is from mean
- Covariance is a measure of how much 2 random vars vary together

## Covariance Matrix
- a square matrix Ci,j = sigma(xi,xj) where C in R^dxd , d num of random vars (features)
- Symmetrical
- The diagonal entries of the covariance matrix are the variances
```
Covariance Matrix =
[*sig(x,x)* sig(x,y)]
[sig(y,x) *sig(y,y)*]

These are indeed the variances: `Var(x)` and `Var(y)`.
```


## SVD (Singular Value Decomposition)
- SVD is a powerful matrix factorization technique that decomposes any rectangular matrix `A` into three other matrices:  
	`A = U Σ V^T`

**Point/Importance of SVD:**  
SVD is incredibly useful in various fields like data science, image processing, and recommender systems. It allows you to:

- **Dimensionality Reduction:** By keeping only the largest singular values, you can approximate the original matrix with fewer dimensions, effectively compressing data or reducing noise (e.g., in PCA).
- **Data Compression:** Representing complex data with fewer components.
- **Feature Extraction:** Identifying the most important features in a dataset.
- **Noise Reduction:** Separating signal from noise.
## Anatomy of SVD
`A = U Σ V^T`  
Where:

- `U`: An `m x m` **orthogonal matrix** whose columns are the **left singular vectors** of A. These are the eigenvectors of `A A^T`.
- `Σ` (Sigma): An `m x n` **diagonal matrix** containing the **singular values** of A on its diagonal. The singular values (σ) are the square roots of the non-negative eigenvalues (λ) of `A^T A` (or `A A^T`), arranged in descending order.
- `V^T`: An `n x n` **orthogonal matrix** (transpose of V) whose rows are the **right singular vectors** of A. These are the eigenvectors of `A^T A`.
## Calculation in SVD

### EX:
- Consider Matrix A = 
```
[3 2 2]
[2 3-2]
```
- A^T A =
```
[3 2]
[2 3] [3 2 2]
[2-2] [2 3-2]
```
- λ1 = 25,  
- λ2=9, 
- λ3=0
	- eigenvalues
- eigenvectors =?

- σ1 = sqrt(25) = 5
- σ2 = sqrt(9) = 3
- σ3 = sqrt(0) = 0

- For U, ui = 1/σi \* Avi
- u1 =
```
1/5 *
[]
```

**Finally**
- A = U SUM(V^T)
- (2x3) = (2x2)(2x3)(3x3)


