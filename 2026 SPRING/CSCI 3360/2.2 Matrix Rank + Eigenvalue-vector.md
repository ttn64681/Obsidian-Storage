```table-of-contents
```
# Big Data
- **data that's high-dimensional**
	- high volume
- as dimensions increase, data becomes sparse
	- "More features aren't always better"
	- **Distance metrics (Euclidean) lose meaning**
	- **Models overfit (memorizing noise instead of signal)**

- you can have 2 points similar in one dimension (x and z), but not in another 2 dimensions (y and z)
- that's why more dimensions make it harder to define similarity
	- this causes overfitting, since it's hard to find similarities, and instead memorize
		- model underperforms/fails

## What is Information?
- in Linear Algebra, information ~= Variance.
- If a feature (column) never changes (variance = 0), it tells us nothing
- If 2 features are perfectly correlated, one is redundant
- **Rank** helps us measure this

## Matrix Rank
- **Rank:** the num of *linearly independent* rows or cols
- **Linear Independence:**
	- a set of vectors in Linearly Independent if *no vector can be built from a combination of the others*
- How do you find it?

- the num of non-zero rows in its **row echelon form**, which corresponds to the number of picots
	- row echelon form of matrix represents 0 vectors/rows
	- 0 rows at bottom of matrix
	- for each col/feature try to get pivots, that would be the non-zero val, and 
	- reduce it to 1s and 0s
- How do you transform a matrix into it's row echelon form?

## Revisiting Solving Linear Systems (Gaussian Elimination)
2x + 4y - 2z = 14
x + 3y + z = 11
3x + 2y + z = 11

->
(Matrix/Augmented form)

| 2   | 4   | -2  | 13  |
| --- | --- | --- | --- |
| 1   | 3   | 1   | 11  |
| 3   | 2   | 1   | 11  |
- swap rows
- Create the first pivot:
	- R1 <-> R2
->

| *1*   | *3*   | *1*   | *11*  |
| --- | --- | --- | --- |
| *2*   | *4*   | *-2*  | *13*  |
| 3   | 2   | 1   | 11  |
- Eliminate below the first pivot:
	- R2 <- R2 - 2R1
	- R3 <- R3 - 3R1
->

| 1     | 3      | 1      | 11     |
| ----- | ------ | ------ | ------ |
| **0** | **-2** | **-4** | **-9** |
| **0**     | **-7**     | **-2**     | **-22**    |
- Create the second pivot:
	- R2 <- -½ R2

| 1     | 3     | 1     | 11      |
| ----- | ----- | ----- | ------- |
| **0** | **1** | **2** | **4.5** |
| 0     | -7    | -2    | -22     |
- Eliminate below the second pivot:
	- R3 <- R3 + 7R2
->

| 1     | 3     | 1     | 11      |
| ----- | ----- | ----- | ------- |
| **0** | **1** | **2** | **4.5** |
| 0     | 0     | 12    | 6       |
- **This is our Row Echelon Form** 
- Row echelon form conditions:
	- All rows consisting entirely of zeros are at bottom of matrix



- From the augmented matrix,
	- x + 3y + z = 11
	- y + 2z = 4.5
	- 12z = 6

- Use back-substitution to find the values of **x,y,z**

## How does Row Echelon Form help in calculating Rank?
### Row Echelon Transformation
- R2 <- R2 - 4R1
- R3 <- R3 - 5R1
- R3 <- R3 - R2

A = 

| 1   | 2   | 3   |
| --- | --- | --- |
| 4   | 5   | 6   |
| 5   | 7   | 9   |
- which row(s) are dependent on other rows?
- After simplification, we discover that **one of the rows is a linear combination of the others, making it redundant**. (579)
- after you simplify them to row echelon transformation,
	- you discover there are only 2 rows that are linearly independent, so the **rank is 2**.
- This reveals the **true intrinsic dimensionality** of the data, indicating the effective number of independent "features" or "dimensions."
	- Thus, we have reduces noise and redundancy
	- reducing overfitting and computational cost
->

| 1   | 2   | 3   |
| --- | --- | --- |
| 0   | -3  | -6  |
| 0   | -3  | 9   |
->

| 1   | 2   | 3   |
| --- | --- | --- |
| 0   | -3  | -6  |
| 0   | 0   | 0   |

# Eigenvalues & Eigenvalues
- Eigen roughly translates to "own" or characteristics
- Eigenvalues and eigenvectors are only for square matrices
- Eigenvectors are by definition non-zero
- Eigenvalues may be equal to zero

- Let A be an nxn matrix
	- an Eigenvector of A is a **nonzero vector** *v* in R^n such that Av = λv, for some scalar λ
	- An Eigenvalue of A is a scalar *λ* such that the equation Av = λv has a nontrivial solution
- If Av = λv for vA = 0, we say that λ is the eigenvalue for v, and that v is an eigenvector for λ.

- for a matrix, it could have eigenvalues (not always), but if you do, you also have eigenvectors
- for a problem, you'd be given just the matrix, and from that you have to find the eigenvalues and eigenvectors
- you calc this by doing the equation Av = λv

- They identify _what those principal independent dimensions are_ (eigenvectors) and _how much variance each dimension accounts for_ (eigenvalues). This is crucial for reducing high-dimensional data by keeping only the eigenvectors associated with the largest eigenvalues, effectively reducing noise and redundancy while retaining the most important information.

## Finding Eigenvalues & Eigenvectors of a Matrix
- consider the matrix and vectors
- A =
[2 2]
[-4 8]
- v =
[1]
[1]
- w = 
[2]
[1]

- Av = λv
- 



- Consider Matrix A = 

| 4   | 1   |
| --- | --- |
| 2   | 3   |
- Av = λv => (A-λI)v = 0
- For a non-zero vector v to exist, the matrix (A - λI) must be Singular (non-inevitable), i.e., det(A - λI) = 0

Matrix A = 

| 4   | 1   |
| --- | --- |
| 2   | 3   
,
A - λI = 

| 4 - λ | 1     |
| ----- | ----- |
| 2     | 3 - λ |
- det(A - λI) = (4 - λ) (3 - λ) - (1) (2) = 0
	- λ^2 -7λ + 12-2 = 0
	- λ^2 - 7λ + 10 = 0
	- (λ-5)(λ-2 = 0
	- λ1 = 5, λ2 = 2

- Finding v :
	- For λi = 5,
```
[-1 1] [x]   [0]
[2 -2] [y] = [0]
```

```
-x + y = 0 }
2x -2y = 0 } -> y = -2x
```

- Check if the product is a scalar multiple of the vector
	- Av 


- Fail-safe identification - face identification
- instead of having all features, if you just get the eigenvectors and eigenvalues, the most important features(columns)/pixels of the face, then you can recognize the face

- basically we covered today the issue with high dimensionality, so we need to reduce the features and dimensions
- we do this efficiently by identifying the important rows/cols via row echelon form
	- since data is just matrices

- using eigenvalues, we generate eigenvectors, we generate "Principal Axes" of the data
- Useful in PCA, SVD

# Singular Value Decomposition (SVD)
- SVD works on any matrix A in R^(mxn)
- It is the foundation of PCA (Principal Component Analysis)
- Image Compression
- Recommender Systems
- Denoising Data

## Anatomy of SVD
- A = U ∑ V^T
	- A (mxn): original data
	- U (mxm): "Left Singular Vectors." (Orthonormal).
		- Describes the Sample Space
	- ∑ (mxn): "Singular Values" (Diagonal).
		- Describes the Strength/Importance.
	- V^T (nxn): "Right Singular Vectors." (Orthonormal).
		- Describes the Feature space



# Notes
- you can have 2 points similar in one dimension (x and z), but not in another 2 dimensions (y and z)
- that's why more dimensions make it harder to define similarity
	- this causes overfitting, since it's hard to find similarities, and instead memorize
		- model underperforms/fails

# Vocab
