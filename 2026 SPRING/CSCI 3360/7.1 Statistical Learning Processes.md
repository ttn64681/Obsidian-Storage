```table-of-contents
```
# Intro
Main
1. Supervised Learning
	1. given labels, you learn the data/features
2. Unsupervised Learning
	1. no labels on data, must come up with own object

3. Semi
4. Transfer
5. Reinforced

## Learning from the data
- What is "learning"?
- "...learning is about predicting the future based on the past"
- **Past:** Training data
	- Fully labeled, unsupervised, semi-supervised
	- Classification, Regression
- **Future:** Testing data
	- 
- **Predicting**: Model
	- Linear mode, kernel method, nearest neighbor, decision trees, neural networks

## Statistical Learning
- Input var X ~ predictive, independent var
- Output var Y ~ response, dependent var

1. **Modeling Reality:** When you try to build a model `Y = f(X)` to explain an output variable `Y` based on input variables `X`, there's almost always some part of `Y` that `f(X)` cannot explain perfectly. This unexplained part is the "error" or "noise."
2. **Assumption:** Assuming this error is Gaussian means that the errors are symmetrically distributed around zero (most errors are small, large errors are less common), and their distribution can be described by a bell curve.
3. **Why it matters:**
    - **Simplicity:** It's a common and mathematically convenient assumption.
    - **Statistical Properties:** Many statistical methods (like Ordinary Least Squares regression) perform optimally when errors are normally distributed, especially for constructing confidence intervals and performing hypothesis tests.
    - **Real-world:** Often, the accumulation of many small, independent random factors can lead to an overall error distribution that is approximately Gaussian (Central Limit Theorem).


# Why estimate f?
1. prediction
2. inference

### **Prediction**
- in many situations, a set of inputs X are readily available, but the output Y cannot be easily obtained
			Y^ = f^(X)
- Reducible and irreducible error

- E(Y - Y^2) = E[f(X) + epsilon - f^X]^2
			= [f(X) - f^(X)]^2 + Var(epsilon)]
				*> Reducible*          *> Irreducible*

<span style="background:#fff88f">- predicting sales based on advertising budget/values</span>
- combined predicting the y
### **Inference**
- we are often interested in understanding the association b/w Y and X1, .. , Xp.
- In this situation we wish to estimate f,  but our goal is not necessarily to make predictions for Y
	- Which predictors are associated w/ the response?
	- What is the relationship b/w the response and each predictor?
	- Can the relationship b/w Y and each predictor be adequately summarized?

<span style="background:#fff88f">- how is advertising and sales related?</span>

### Case Study:


# How do we estimate f?
### Parametric methods
- Two-step model-based approach
	- Make an assumption about the functional form or shape of f
	- After a model is selected, fit the model to estimate parameters B0, B1, ... , Bp

	**f(X) = B0 + B1X1 + B2X2 + ... + BpXp**

- fitting data based on data
- tilting the plane to fit model with least mean error, comparing with the data
- we train the models to estimate the coefficients

- the model might not be good


### Non-parametric methods
- we don't look for coefficients
- we try to fit a model that goes close to the data wew have
- Seek an estimate of that gets as close to data points as possible
- avoids assumption of a particular function form of f

- spline fit methods (non-parametric method) which is not a linear plane
- in 2d its a curve, in 3d, its a spline


# Which method to use in analysis?
- if you care about details of ^f(X)? => 
	- you want to perform statistical inference
- Fine w/ treating ^f(X) as a black-box? =>
	- Your interest is prediction

## Prediction flexibility vs Model interpretability
- Prediction accuracy depends on model flexibility
	- Simpler the parametric form of model -> The easier it is to interpret

![[Drawing 2026-02-23 13.56.44.excalidraw]]
- E.g. Deep Learning - High flexibility, low interpretability:
	- you can extract weights, but doesn't say that the weights of this feature are more important than these other weights

- **Parametric models**, for which we can write down a math expression for f(X) before observing the data, a priori (e.g., a linear regression), **are inherently less flexible**
	- popular in analysis


# Notes:
- **Parametric models**, for which we can write down a math expression for f(X) before observing the data, a priori (e.g., a linear regression), **are inherently less flexible**
- 